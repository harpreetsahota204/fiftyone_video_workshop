{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/fiftyone_video_workshop/blob/main/workshop.ipynb)\n",
        "\n",
        "# Understanding Video Data at Scale\n",
        "### A Hands-On Workshop with Action100M and FiftyOne\n",
        "\n",
        "---\n",
        "\n",
        "Video is a hard modality to work with. \n",
        "\n",
        "You're dealing with more data, temporal complexity, and annotation workflows that don't scale. This notebook tackles a practical question: **given a large video dataset, how do you understand what's in it without manually watching thousands of clips?**\n",
        "\n",
        "We're working with a subset of **[Action100M preview](https://www.arxiv.org/abs/2601.10592)**. In this subset there are 1,144 YouTube videos, each clipped to 90 seconds, annotated with a hierarchical *Tree-of-Captions* structure produced by a fully automated AI pipeline: V-JEPA 2 for segmentation, PerceptionLM-3B and Llama-3.2-Vision-11B for captioning, and GPT-OSS-120B with multi-round Self-Refine for structured annotation extraction.\n",
        "\n",
        "Every label in this dataset was written by a model. None of it was seen by a human annotator.\n",
        "\n",
        "As AI-generated datasets become the norm, **the skill of interrogating machine-generated annotations is increasingly important**. This notebook shows you how to do that systematically.\n",
        "\n",
        "---\n",
        "\n",
        "### What We'll Build\n",
        "\n",
        "|  | Question | Tools |\n",
        "|---|---|---|\n",
        "| 1. What We Were Given | What does this dataset claim to contain? | FiftyOne App |\n",
        "| 2. Three Lenses | What does the raw data actually look like? | Qwen3-VL-Embedding, Molmo2, Sentence Transformers |\n",
        "| 3. The Second Opinion | Does a second model agree with the first? | Qwen3-VL |\n",
        "| 4. Measuring Agreement | How much do they agree, per sample? | Text Evaluation Plugin |\n",
        "| 5. Grounding the Hard Cases | Where they disagree, who's right? | Molmo2 |\n",
        "| 6. The Payoff | What can I now do with this? | FiftyOne App |\n",
        "\n",
        "By the end, you'll have a **confidence map** of the dataset's annotations and a reusable workflow for understanding any video dataset with AI-generated labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "fo.config.requirement_error_level=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 1: What We Were Given\n",
        "\n",
        "Before running any models, let's understand the shape of what we have. \n",
        "\n",
        "The Action100M preview comes with rich pre-existing annotations from the Tree-of-Captions pipeline. Each video has temporal segments annotated at multiple levels of granularity:\n",
        "\n",
        "- **Level 0 (root/tier):** The full video — one annotation covering the entire 90-second clip\n",
        "- **Mid levels:** Sub-segments — multi-second chunks describing coherent activities\n",
        "- **Leaf level:** The finest grain — individual moments\n",
        "\n",
        "At each level, there are five annotation fields:\n",
        "- `gpt_summary_brief` — one-sentence clip caption\n",
        "- `gpt_summary_detailed` — full play-by-play description\n",
        "- `gpt_action_brief` — short verb phrase (\"spread almonds on tray\")\n",
        "- `gpt_action_detailed` — instruction-manual version of the action\n",
        "- `gpt_action_actor` — who's performing it\n",
        "\n",
        "All of these were generated by GPT-OSS-120B with three rounds of Self-Refine. We're going to take these at face value for now — and then build the tools to interrogate them.\n",
        "\n",
        "You can either download the dataset from the [Voxel51 Hugging Face org](https://huggingface.co/datasets/Voxel51/action100m_tiny_subset), or if you face rate limit issues, you can follow the instructions in the `download_scripts` directory of this repository to download the dataset and parse it into FiftyOne format.\n",
        "\n",
        "If you are downloading from the Hugging Face Hub, run:\n",
        "\n",
        "```python\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\n",
        "    \"Voxel51/action100m_tiny_subset\",\n",
        "    dataset_name=\"action100m\",\n",
        "    overwrite=True,\n",
        "    persistent=True,\n",
        ")\n",
        "```\n",
        "\n",
        "If you've downloaded the dataset and parsed manually, then run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "dataset = fo.load_dataset(\"action100m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "471823d7",
      "metadata": {},
      "source": [
        "A **[FiftyOne dataset](https://docs.voxel51.com/user_guide/using_datasets.html)** is the core data structure you work with in [FiftyOne](https://docs.voxel51.com/user_guide/basics.html). It:\n",
        "\n",
        "- Logically represents your visual data (images, videos, point clouds, etc.) along with all associated information: labels, metadata, predictions, and other fields.\n",
        "\n",
        "- Is stored in a lightweight, non-relational database (MongoDB) so it can scale to large datasets without loading all media into RAM. \n",
        "\n",
        "- Can be created from many sources: directories of files, common dataset formats (like COCO), or the built‑in Dataset Zoo. \n",
        "\n",
        "Conceptually, you can [think of a dataset like a table in pandas](https://github.com/voxel51/fiftyone/blob/develop/docs/source/tutorials/pandas_comparison.ipynb): it has **rows and columns**, but specialized for computer vision:\n",
        "\n",
        "- In pandas: **row = record**, **column = feature**\n",
        "- In FiftyOne: **sample = record**, **field = feature** \n",
        "\n",
        "You can inspect the schema of a dataset by calling it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26754040",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7ab8fb",
      "metadata": {},
      "source": [
        "### What is a sample?\n",
        "\n",
        "A [**sample**](https://docs.voxel51.com/user_guide/basics.html#samples) is the atomic element of a FiftyOne dataset. It is:\n",
        "\n",
        "- One “item” of your data (for example, a single image or a single video) plus everything you know about it. \n",
        "- A flexible container of [**fields**](https://docs.voxel51.com/user_guide/basics.html#fields), which can include:\n",
        "  - `filepath` to the media\n",
        "  - `metadata` (dimensions, duration, etc.)\n",
        "  - Ground truth labels (detections, classifications, segmentations, etc.)\n",
        "  - Model predictions\n",
        "  - Tags, scalar values, strings, arrays, and more \n",
        "\n",
        "So, **a dataset is a collection of samples**, and **each sample is everything you care about for one media file**, stored in a structured way that’s easy to query, visualize, and modify.\n",
        "\n",
        "You can see what a sample looks like by inspecting the [first](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first) sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec0d626",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5436d44",
      "metadata": {},
      "source": [
        "Once you have your dataset, you can [launch the app](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app) and see what's in it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57635f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open the FiftyOne App and explore the dataset\n",
        "session = fo.launch_app(dataset, auto=False)\n",
        "session.url"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cad16bf7",
      "metadata": {},
      "source": [
        "**What to look at in the App:**\n",
        "\n",
        "1. Play a few videos and toggle on `gpt_action_brief` — watch how the temporal segments track the actions you see on screen\n",
        "2. Click on a sample with high `tree_depth` (try sorting by it) — notice how the annotations at level 0 are broad overviews, while leaf-level annotations are very fine-grained moments\n",
        "3. Look at the `transcript` field — this is the raw ASR text, much noisier than the GPT-refined annotations\n",
        "4. Check a few `gpt_summary_detailed` labels — these are the longest annotations (~540 words average)\n",
        "\n",
        "> **Key question to hold in mind:** Every one of these labels came from an automated pipeline. They look authoritative. But should we trust them?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0244b7",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 2: Three Lenses on the Same Data\n",
        "\n",
        "Before interrogating the existing annotations, let's build our own understanding of the dataset — using nothing but the raw videos and transcripts.\n",
        "\n",
        "We'll create three separate embedding spaces:\n",
        "\n",
        "1. **Visual ([Qwen3-VL-Embedding](https://docs.voxel51.com/plugins/plugins_ecosystem/qwen3vl_embeddings.html)):** What the videos look like — and crucially, this lives in a shared text-video space, so we can search with natural language\n",
        "\n",
        "2. **Visual-Grounding ([Molmo2](https://docs.voxel51.com/plugins/plugins_ecosystem/molmo2.html)):** A different visual understanding — video-to-video similarity only, but from a model trained on grounding and spatial reasoning\n",
        "\n",
        "3. **Language (Transcript):** What people are *saying* in these videos, embedded with a text model\n",
        "\n",
        "The insight from comparing these three spaces is the foundation of everything that follows: **what you embed determines what you find**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036f351b",
      "metadata": {},
      "source": [
        "### Lens 1: Visual Content (Qwen3-VL-Embedding)\n",
        "\n",
        "[Qwen3-VL-Embedding](https://huggingface.co/Qwen/Qwen3-VL-Embedding-2B) maps video and text into a shared vector space. This means we can embed a natural language query and find videos that match without touching any of the existing labels. It's semantic search, not keyword search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af062de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the 2B embedding model — good balance of quality and speed\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/qwen3vl_embeddings\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "qwen_emb_model = foz.load_zoo_model(\n",
        "    \"Qwen/Qwen3-VL-Embedding-2B\",\n",
        ")\n",
        "\n",
        "qwen_emb_model.max_length=32768\n",
        "\n",
        "# Compute embeddings — stores a vector on each sample\n",
        "dataset.compute_embeddings(\n",
        "    qwen_emb_model,\n",
        "    embeddings_field=\"qwen_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=8,\n",
        "    skip_failures=False,\n",
        ")\n",
        "\n",
        "print(f\"Embeddings computed for {dataset.exists('qwen_embeddings').count()} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6907630f",
      "metadata": {},
      "source": [
        "### What you can do after computing embeddings in FiftyOne\n",
        "\n",
        "Once you've computed embeddings, you unlock powerful workflows:\n",
        "\n",
        "#### Visualize your embeddings in the App\n",
        "\n",
        "[Use `compute_visualization()`](https://docs.voxel51.com/brain.html#visualizing-embeddings) to reduce embeddings with UMAP, t‑SNE, or PCA and explore them in the Embeddings panel to:\n",
        "- See clusters and data structure  \n",
        "- Understand how classes group together  \n",
        "- Lasso regions to create views and filter subsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33179678",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "# Project into 2D with UMAP for visualization\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"qwen_viz\",\n",
        "    embeddings=\"qwen_embeddings\",\n",
        "    num_dims=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db429e1d",
      "metadata": {},
      "source": [
        "\n",
        "####  Search by visual similarity\n",
        "\n",
        "[Build a similarity index](https://docs.voxel51.com/brain.html#similarity) with `compute_similarity()` to:\n",
        "- Find similar images (image‑to‑image search)  \n",
        "- Detect duplicates or unique samples  \n",
        "- Enable visual search in the App or via Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a14462",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a similarity index — this is what powers text-to-video search\n",
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    brain_key=\"qwen_sim\",\n",
        "    embeddings=\"qwen_embeddings\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "967bae59",
      "metadata": {},
      "source": [
        "You can also use this model for zero-shot classification. Let's add a Field to the Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ebd6a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = [\n",
        "    \"Cooking and Food\",\n",
        "    \"Home Improvement and DIY\",\n",
        "    \"Health and Beauty\",\n",
        "    \"Hobbies and Crafts\",\n",
        "    \"Sports and Fitness\",\n",
        "    \"Gardening\",\n",
        "    \"Technology and Electronics\",\n",
        "    \"Fashion and Style\",\n",
        "    \"Arts and Music\",\n",
        "    \"Automotive\",\n",
        "    \"Pets and Animals\",\n",
        "    \"Education and Learning\"\n",
        "]\n",
        "\n",
        "# Configure model for classification\n",
        "qwen_emb_model.classes = classes\n",
        "qwen_emb_model.text_prompt = \"A video about \"\n",
        "\n",
        "# Apply zero-shot classification\n",
        "dataset.apply_model(\n",
        "    qwen_emb_model, \n",
        "    label_field=\"predicted_class\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lens 2: Visual-Grounding (Molmo2)\n",
        "\n",
        "Molmo2 is a different kind of vision-language model — it's trained heavily on grounding tasks: pointing to objects, tracking them through time, and counting. Its internal representations will weight spatial and motion features differently than Qwen3-VL-Embedding.\n",
        "\n",
        "We can't do text-to-video search with Molmo2 embeddings, but we can still visualize the embedding space and do video-to-video similarity. \n",
        "\n",
        "The question is: **do two different visual models agree on which videos are similar?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz \n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/molmo2\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "molmo_model = foz.load_zoo_model(\"allenai/Molmo2-4B\")\n",
        "\n",
        "molmo_model.pooling_strategy = \"mean\"\n",
        "\n",
        "dataset.compute_embeddings(\n",
        "    molmo_model,\n",
        "    embeddings_field=\"molmo_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=4,\n",
        "    skip_failures=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"molmo_viz\",\n",
        "    embeddings=\"molmo_embeddings\",\n",
        "    num_dims=2,\n",
        ")\n",
        "\n",
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    brain_key=\"molmo_sim\",\n",
        "    embeddings=\"molmo_embeddings\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lens 3: Language (Transcript Embeddings)\n",
        "\n",
        "Both visual embeddings above represent what you *see* in the videos. Now let's embed what you *hear* — the ASR transcript text.\n",
        "\n",
        "Instructional videos vary enormously in how much they narrate. Some presenters talk through every step; others work in silence. The transcript embedding space captures this variation in a way neither visual model can.\n",
        "\n",
        "Fo this we can make use of [`jina-embeddings-v5-text-small-clustering`](https://huggingface.co/jinaai/jina-embeddings-v5-text-small-clustering). \n",
        "\n",
        "This model is not integrated as a remote source zoo model, but we can make use of it by extracting the transcripts from the videos using the `values` method of the [Dataset](https://docs.voxel51.com/user_guide/basics.html#datasets) to get all the values of the [Field](https://docs.voxel51.com/user_guide/basics.html#fields) into a Python list.\n",
        "\n",
        "With the values in a list we can use the model natively and then add the results back as a Field to the Dataset using [the `set_values`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.set_values) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#set an environment variable so tokenizers doesn't yell at us,\n",
        "# note this related to the `transformers` and `tokenizers` libraries and not a FiftyOne specific environment variable\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "text_emb_model = SentenceTransformer(\n",
        "    \"jinaai/jina-embeddings-v5-text-small-clustering\",\n",
        "    model_kwargs={\"dtype\": torch.bfloat16}, #recommended for GPUs\n",
        "    config_kwargs={\"attn_implementation\": \"flash_attention_2\"}, #optional, but recommended\n",
        "    tokenizer_kwargs={\"extra_special_tokens\": {}}, # This line fixes the AttributeError: 'list' object has no attribute 'keys'\n",
        ")\n",
        "\n",
        "transcripts = dataset.values(\"transcript\")\n",
        "\n",
        "# Encode texts\n",
        "text_embeddings = text_emb_model.encode(transcripts)\n",
        "\n",
        "dataset.set_values(\"text_embeddings\", text_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e64caaf",
      "metadata": {},
      "source": [
        "We can compute visualizations of the embeddings just like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f245ca5",
      "metadata": {},
      "outputs": [],
      "source": [
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"transcript_viz\",\n",
        "    embeddings=\"text_embeddings\",\n",
        "    num_dims=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6cb543b",
      "metadata": {},
      "source": [
        "### Using Embeddings to Compute Uniqueness and Representativeness Values\n",
        "\n",
        "In FiftyOne, both **uniqueness** and **representativeness** are scalar scores computed from embeddings that describe how a sample relates to the rest of the dataset.\n",
        "\n",
        "#### Uniqueness\n",
        "\n",
        "- [Measures **how different (dissimilar)**](https://docs.voxel51.com/brain.html#image-uniqueness) a sample is from its neighbors in embedding space.  \n",
        "\n",
        "- Implemented by looking at each sample’s nearest neighbors, weighting their distances (e.g. 60%-30%-10%), and normalizing to a score in **\\[0, 1\\]**. Higher scores mean the sample is more “isolated” or distinct; lower scores mean it has many close neighbors.\n",
        " \n",
        "- The most unique sample in a collection has a uniqueness value of **1**.\n",
        "\n",
        "- Useful for:\n",
        "  - Finding **outliers / edge cases / bad actors**  \n",
        "  - Detecting **near-duplicates** (by looking at *low* uniqueness)  \n",
        "  - Selecting **diverse samples** for annotation when budget is limited \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ae91c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_uniqueness(\n",
        "    dataset,\n",
        "    uniqueness_field = \"qwen_uniqueness\",\n",
        "    embeddings = \"qwen_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fcd344b",
      "metadata": {},
      "source": [
        "## Representativeness\n",
        "\n",
        "- [Measures **how typical** or **central** ](https://docs.voxel51.com/brain.html#image-representativeness) a sample is relative to the rest of the dataset in embedding space.  \n",
        "\n",
        "- Also normalized to **\\[0, 1\\]**, with the most representative samples having value **1**.  \n",
        "\n",
        "- High representativeness ≈ sample lies in a dense, central region of the data; it’s similar to many other samples.  \n",
        "\n",
        "- Useful for:\n",
        "  - **Active learning**: picking representative samples to cover common patterns  \n",
        "  - **Dataset balancing**: finding under/overrepresented regions  \n",
        "  - **Efficient annotation**: prioritizing samples that “stand in” for many others \n",
        "\n",
        "In short:\n",
        "\n",
        "- **Uniqueness** → “How unusual is this sample?”  \n",
        "- **Representativeness** → “How well does this sample represent many others?”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad16d93",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_representativeness(\n",
        "    dataset,\n",
        "    representativeness_field = \"qwen_rep\",\n",
        "    embeddings = \"qwen_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Cross-Lens Moment\n",
        "\n",
        "Try to **find a tight visual cluster in the Qwen UMAP, then look at where those same videos land in the transcript UMAP.**\n",
        "\n",
        "You may find that videos which cluster tightly by visual content are often spread out in transcript space. Cooking videos that all look similar — same kitchen setup, same hands-on-food visual — may use completely different spoken language: detailed step-by-step narration, casual conversational chat, or total silence.\n",
        "\n",
        "This is not a failure of any embedding model, but a property of the data: **visual similarity and linguistic similarity are measuring different things.** Your choice of embedding determines what questions you can ask.\n",
        "\n",
        "- Qwen visual embeddings to find *visually similar content* or do text-to-video semantic search\n",
        "\n",
        "- Molmo2 embeddings to find videos with similar *physical actions and spatial structure*\n",
        "\n",
        "- Transcript embeddings to find videos that *talk about similar topics*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick a video and find its nearest neighbors in each embedding space — \n",
        "# a concrete way to see how the three spaces differ\n",
        "sample = dataset.first()\n",
        "print(f\"Reference video: {sample.title}\")\n",
        "print(f\"Root annotation: {sample.gpt_root_summary}\\n\")\n",
        "\n",
        "for brain_key, label in [(\"qwen_sim\", \"Qwen visual\"), (\"molmo_sim\", \"Molmo2 visual\")]:\n",
        "    neighbors = dataset.sort_by_similarity(sample.id, brain_key=brain_key, k=4)\n",
        "    print(f\"Nearest neighbors ({label}):\")\n",
        "    for n in neighbors.skip(1).take(3):\n",
        "        print(f\"  → {n.gpt_root_summary}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e305612",
      "metadata": {},
      "source": [
        "**What to look at:** \n",
        "\n",
        "- In the Embeddings panel, hover over clusters. Click points to see which videos land near each other. Try coloring by `tree_depth` or `title` to see if the visual clusters map onto any metadata patterns.\n",
        "\n",
        "\n",
        "- Compare the Molmo2 UMAP with the Qwen UMAP. Are the clusters in the same positions? Similar shape, different boundaries — this is two models with different visual priors producing different embedding spaces. Neither is \"correct\" — they're measuring different aspects of the same content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 3: The Second Opinion\n",
        "\n",
        "We've built three independent maps of the dataset. Now let's do what the original pipeline did — but with a completely different model.\n",
        "\n",
        "The Action100M annotations were produced by GPT-OSS-120B processing outputs from PerceptionLM-3B and Llama-3.2-Vision-11B. \n",
        "\n",
        "We're going to run **Qwen3-VL-8B** on the same videos, completely independently, and ask:\n",
        "\n",
        "- How would *you* describe this video?\n",
        "\n",
        "- What events do *you* see, and when?\n",
        "\n",
        "We're not trying to beat the pipeline. We're trying to get a second opinion. Where the two agree, we gain confidence in the original annotation. Where they diverge, we've found something worth looking at."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz \n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/qwen3vl_video\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "qwen_video_model = foz.load_zoo_model(\n",
        "    \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b61fd92",
      "metadata": {},
      "source": [
        "#### Generate detailed video description\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a full-video description for every sample.\n",
        "# This produces a string field: qwen_desc_summary\n",
        "qwen_video_model.operation = \"description\"\n",
        "\n",
        "dataset.apply_model(\n",
        "    qwen_video_model,\n",
        "    label_field=\"qwen_desc\",\n",
        "    skip_failures=True,\n",
        ")\n",
        "\n",
        "print(f\"Descriptions generated: {dataset.exists('qwen_desc_summary').count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e915724",
      "metadata": {},
      "source": [
        "#### Comprehensive analysis\n",
        "\n",
        "Analyzes video for all aspects: description, events, objects, scene info, activities.\n",
        "\n",
        "Output fields:\n",
        "\n",
        "- `analysis_summary` - Video description (string)\n",
        "- `analysis_events` - Temporal events (fo.TemporalDetections)\n",
        "- `analysis_objects` - Object appearances (fo.TemporalDetections)\n",
        "- `analysis_scene_info_*` - Scene classifications\n",
        "- `analysis_activities_*` - Activity classifications\n",
        "- `sample.frames[N].objects` - Frame-level object detections\n",
        "- `sample.frames[N].text_content` - Frame-level OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qwen_video_model.operation = \"comprehensive\"\n",
        "\n",
        "dataset.apply_model(\n",
        "    qwen_video_model,\n",
        "    label_field=\"qwen_comp\",\n",
        "    skip_failures=True,\n",
        ")\n",
        "\n",
        "print(f\"Temporal events generated: {dataset.exists('qwen_events').count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice the descriptions often describe the same video in quite different language — different vocabulary, different sentence structure, different level of detail. Sometimes one catches a detail the other misses. \n",
        "\n",
        "The question is: **how do we move from \"these look different\" to a number we can sort and filter by?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fecac697",
      "metadata": {},
      "source": [
        "#### Using a prompt from the paper\n",
        "\n",
        "In the paper they describe some prompts they use. We can prompt Qwen3-VL the same way and see what we end up with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a5e7a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a full-video description for every sample.\n",
        "# This produces a string field: qwen_desc_summary\n",
        "qwen_video_model.operation = \"custom\"\n",
        "\n",
        "qwen_video_model.prompt = \"\"\"Identify the main actor and the physical action performed in the current segment. Provide both a brief\n",
        "description that represents the overall action step, and a detailed description that contains sufficient\n",
        "procedural detail. Use \"N/A\" (without further explaination) if there are no visible actors or physical\n",
        "actions (e.g., static).\n",
        "\n",
        "# Response Formats\n",
        "## output\n",
        "{\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"summary\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"brief\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Single sentence video caption.\"\n",
        "                },\n",
        "                \"detailed\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Detailed, comprehensive description.\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"action\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"brief\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"A single verb phrase (no -ing forms) brifly summarizing the overall action content.\"\n",
        "                },\n",
        "                \"detailed\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"A single imperitive sentence describing how the action is performed with more details.\"\n",
        "                },\n",
        "                \"actor\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Single sentece or an imformative noun phrase describing who is performing the action.\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"summary\", \"action\"]\n",
        "}\"\"\"\n",
        "\n",
        "\n",
        "dataset.apply_model(\n",
        "    qwen_video_model,\n",
        "    label_field=\"qwen_custom\",\n",
        "    skip_failures=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "128fc6d1",
      "metadata": {},
      "source": [
        "The model returned its output as a raw JSON string stored in `qwen_custom_result`.\n",
        "\n",
        "We parse the JSON once and promote each nested value to its own top-level string field. \n",
        "\n",
        "This makes every piece of structured output a first-class citizen in the dataset: filterable in the App, embeddable with a text model, and comparable against the existing GPT-generated fields (`gpt_summary_brief`, `gpt_action_actor`, etc.) that ship with the Action100M dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ab545e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def extract_qwen_fields(raw: str) -> tuple:\n",
        "    \"\"\"Parse a qwen_custom_result JSON string and return its leaf values.\n",
        "\n",
        "    The model output follows a JSON Schema-like envelope where actual text\n",
        "    values are stored under properties.<key>.properties.<subkey>. This\n",
        "    function navigates that structure and returns the five text fields we\n",
        "    care about, in order:\n",
        "        (summary_brief, summary_detailed, action_brief, action_detailed, action_actor)\n",
        "\n",
        "    Returns a tuple of five Nones if the string is missing or malformed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parsed = json.loads(raw)\n",
        "        props = parsed[\"properties\"]\n",
        "        s = props[\"summary\"][\"properties\"]\n",
        "        a = props[\"action\"][\"properties\"]\n",
        "        return (\n",
        "            s.get(\"brief\"),\n",
        "            s.get(\"detailed\"),\n",
        "            a.get(\"brief\"),\n",
        "            a.get(\"detailed\"),\n",
        "            a.get(\"actor\"),\n",
        "        )\n",
        "    except (json.JSONDecodeError, KeyError, TypeError):\n",
        "        # Return nulls for any sample whose output was malformed or missing\n",
        "        return None, None, None, None, None\n",
        "\n",
        "\n",
        "# Pull all raw strings in a single read pass — more efficient than\n",
        "# accessing them one sample at a time inside a loop.\n",
        "raws = dataset.values(\"qwen_custom_result\")\n",
        "\n",
        "# Parse every sample up front so we only iterate the list once per field write.\n",
        "results = [extract_qwen_fields(r) for r in raws]\n",
        "\n",
        "# Write each field as a bulk operation. set_values() does one DB write per call,\n",
        "# which is significantly faster than saving inside iter_samples() for larger datasets.\n",
        "dataset.set_values(\"qwen3vl_summary_brief\",    [r[0] for r in results])\n",
        "dataset.set_values(\"qwen3vl_summary_detailed\", [r[1] for r in results])\n",
        "dataset.set_values(\"qwen3vl_action_brief\",     [r[2] for r in results])\n",
        "dataset.set_values(\"qwen3vl_action_detailed\",  [r[3] for r in results])\n",
        "dataset.set_values(\"qwen3vl_action_actor\",     [r[4] for r in results])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1800e15",
      "metadata": {},
      "source": [
        "Each `gpt_summary_brief`, `gpt_summary_detailed`, etc. is a [`TemporalDetections` field](https://docs.voxel51.com/user_guide/using_datasets.html#temporal-detection) containing a list of detections across multiple tiers. \n",
        "\n",
        "We want to \"hoist\" just the root-tier detection's label string up to a flat sample-level field for easy filtering and comparison. To accomplish this we can [use a combination of `ViewField` and `Filtering`](https://docs.voxel51.com/user_guide/using_views.html#querying-samples).\n",
        "\n",
        "This is necessary for the next step in the workshop, where we will compare the Qwen3-VL outputs with the \"ground truth\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b489b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "# Fields to extract root labels from, and their new target field names\n",
        "GPT_FIELDS = {\n",
        "    \"gpt_summary_brief\":    \"gpt_summary_root_brief\",\n",
        "    \"gpt_summary_detailed\": \"gpt_summary_root_detailed\",\n",
        "    \"gpt_action_brief\":     \"gpt_action_root_brief\",\n",
        "    \"gpt_action_detailed\":  \"gpt_action_root_detailed\",\n",
        "    \"gpt_action_actor\":     \"gpt_action_root_actor\",\n",
        "}\n",
        "\n",
        "for src_field, dst_field in GPT_FIELDS.items():\n",
        "    # Create a view that filters each sample's detections to root-tier only.\n",
        "    # This does NOT mutate the dataset — it's a virtual filter.\n",
        "    root_view = dataset.filter_labels(src_field, F(\"tier\") == \"root\")\n",
        "\n",
        "    # values() on a TemporalDetections field returns a list-of-lists:\n",
        "    # one inner list per sample, containing the labels of surviving detections.\n",
        "    # Since there is exactly one root per sample, each inner list has one element.\n",
        "    nested = root_view.values(f\"{src_field}.detections.label\")\n",
        "\n",
        "    # Flatten: take the first (only) label, or None if a sample had no root.\n",
        "    flat = [labels[0] if labels else None for labels in nested]\n",
        "\n",
        "    dataset.set_values(dst_field, flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 4: Measuring Agreement\n",
        "\n",
        "Eyeballing agreement doesn't scale. We need metrics.\n",
        "\n",
        "\n",
        "##### **Normalized Levenshtein Similarity** \n",
        "\n",
        "This metric measures how similar two text strings are on a scale from 0.0 to 1.0.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **Levenshtein Distance**: Counts the minimum number of single-character edits (insertions, deletions, substitutions) needed to transform one string into another.\n",
        "\n",
        "2. **Normalization**: Converts the distance to a similarity score:\n",
        "   \n",
        "$$\n",
        "\\text{similarity} = 1.0 - \\frac{\\text{Levenshtein distance}}{\\max(\\text{len}(s_1), \\text{len}(s_2))}\n",
        "$$\n",
        "\n",
        "\n",
        "3. **Score**: \n",
        "   - **1.0** = perfect match\n",
        "   - **0.0** = completely different\n",
        "   - **0.0-1.0** = partial similarity\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **\"hello\"** vs **\"hello\"** → 1.0 (no edits)\n",
        "- **\"hello\"** vs **\"helo\"** → 0.8 (1 deletion / 5 chars)\n",
        "- **\"hello\"** vs **\"world\"** → 0.2 (4 substitutions / 5 chars)\n",
        "\n",
        "**Use Cases:**\n",
        "\n",
        "Evaluating OCR accuracy, transcription quality, text generation models, or any scenario requiring quantification of text similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.operators as foo\n",
        "\n",
        "sim_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_normalized_similarity\")\n",
        "\n",
        "# Each tuple is (qwen prediction field, gpt ground truth field, output score field)\n",
        "COMPARISON_PAIRS = [\n",
        "    (\"qwen3vl_summary_brief\",    \"gpt_summary_root_brief\",    \"aconf_summary_brief\"),\n",
        "    (\"qwen3vl_summary_detailed\", \"gpt_summary_root_detailed\", \"aconf_summary_detailed\"),\n",
        "    (\"qwen3vl_action_brief\",     \"gpt_action_root_brief\",     \"aconf_action_brief\"),\n",
        "    (\"qwen3vl_action_detailed\",  \"gpt_action_root_detailed\",  \"aconf_action_detailed\"),\n",
        "    (\"qwen3vl_action_actor\",     \"gpt_action_root_actor\",     \"aconf_action_actor\"),\n",
        "]\n",
        "\n",
        "for pred_field, gt_field, output_field in COMPARISON_PAIRS:\n",
        "    sim_op(\n",
        "        dataset,\n",
        "        pred_field=pred_field,\n",
        "        gt_field=gt_field,\n",
        "        output_field=output_field,\n",
        "        case_sensitive=False,\n",
        "        delegate=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48fadb2a",
      "metadata": {},
      "source": [
        "This metric computes **Semantic Similarity** using neural embeddings, measuring whether two texts have the same *meaning* rather than the same *characters*.\n",
        "\n",
        "\n",
        "1. **Sentence Encoding**: Uses a pre-trained neural network (from `sentence-transformer`) to convert each text into a high-dimensional vector (embedding) that captures its meaning.\n",
        "\n",
        "2. **Cosine Similarity**: Computes the angle between the two embedding vectors:\n",
        "   \n",
        "   $$\n",
        "   \\text{similarity} = \\max(0, \\cos(\\text{embedding}_{\\text{gt}}, \\text{embedding}_{\\text{pred}}))\n",
        "   $$\n",
        "\n",
        "3. **Score**: \n",
        "   - **1.0** = semantically identical\n",
        "   - **0.0** = completely unrelated\n",
        "   - **0.0-1.0** = partial semantic overlap\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **\"The car is fast\"** vs **\"The automobile is quick\"** → ~0.95 (same meaning, different words)\n",
        "- **\"hello\"** vs **\"helo\"** → ~0.85 (similar meaning despite typo)\n",
        "- **\"cat\"** vs **\"dog\"** → ~0.6 (related concepts)\n",
        "\n",
        "**Comparison to Normalized Levenshtein Similarity:**\n",
        "\n",
        "**Levenshtein** measures character-level edits using string matching. It's very fast but penalizes any character difference, even if the meaning is preserved. Best for detecting typos, OCR errors, or when exact wording matters.\n",
        "\n",
        "**Semantic** measures meaning using neural embeddings. It's slower (requires model inference) but rewards conceptual equivalence regardless of wording. For example, \"car\" vs \"automobile\" scores 0.0 with Levenshtein but ~0.9 with Semantic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f74622",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.operators as foo\n",
        "\n",
        "sem_op = foo.get_operator(\n",
        "    \"@harpreetsahota/text-evaluation-metrics/compute_semantic_similarity\"\n",
        ")\n",
        "\n",
        "COMPARISON_PAIRS = [\n",
        "    (\"qwen3vl_summary_brief\",    \"gpt_summary_root_brief\",    \"semconf_summary_brief\"),\n",
        "    (\"qwen3vl_summary_detailed\", \"gpt_summary_root_detailed\", \"semconf_summary_detailed\"),\n",
        "    (\"qwen3vl_action_brief\",     \"gpt_action_root_brief\",     \"semconf_action_brief\"),\n",
        "    (\"qwen3vl_action_detailed\",  \"gpt_action_root_detailed\",  \"semconf_action_detailed\"),\n",
        "    (\"qwen3vl_action_actor\",     \"gpt_action_root_actor\",     \"semconf_action_actor\"),\n",
        "]\n",
        "\n",
        "for pred_field, gt_field, output_field in COMPARISON_PAIRS:\n",
        "    sem_op(\n",
        "        dataset,\n",
        "        pred_field=pred_field,\n",
        "        gt_field=gt_field,\n",
        "        model_name=\"all-mpnet-base-v2\",\n",
        "        output_field=output_field,\n",
        "        delegate=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What to look at:** Browse the bottom of the sorted view (lowest confidence). Are these actually bad annotations, or just paraphrases that look different? This is the limitation of lexical metrics — they can't tell the difference. A future extension here is a semantic similarity operator using sentence embeddings, which would score paraphrases much more accurately.\n",
        "\n",
        "For now, the low-confidence samples are exactly what we want for the next section: the cases where the two systems most clearly diverge."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "405f94ba",
      "metadata": {},
      "source": [
        "### Section 5: Grounding the Hard Cases\n",
        "\n",
        "We now have a ranked list of the samples where GPT-OSS-120B and Qwen3-VL disagree most strongly. Instead of reading more text descriptions, let's ask a different question: can a third model show us what's actually happening?\n",
        "\n",
        "Molmo2's core strength is grounding — it can point to specific objects and actors in frames, and it can localize when events happen over time. When two models disagree in words, Molmo2 provides a qualitatively different kind of evidence: spatial coordinates in the actual pixels.\n",
        "\n",
        "This isn't just a tiebreaker. It demonstrates a principle: text descriptions are abstractions. When abstractions conflict, go back to the source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Work on the bottom quartile — the most uncertain samples\n",
        "# threshold = sorted(scores)[len(scores) // 4]  # 25th percentile\n",
        "# uncertain_view = dataset.match(F(\"annotation_confidence\") < threshold)\n",
        "# print(f\"Working with {len(uncertain_view)} low-confidence samples (score < {threshold:.2f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Temporal localization with Molmo2 — independent from both previous models\n",
        "# molmo_model.operation = \"temporal_localization\"\n",
        "\n",
        "# uncertain_view.apply_model(\n",
        "#     molmo_model,\n",
        "#     label_field=\"molmo_events\",\n",
        "#     skip_failures=True,\n",
        "# )\n",
        "\n",
        "# print(f\"Molmo2 temporal localization done for {uncertain_view.exists('molmo_events').count()} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the uncertain samples with all three annotation sources visible:\n",
        "# gpt_action_brief (original), qwen_events (second opinion), molmo_events (third opinion)\n",
        "# molmo_actor keypoints are frame-level — visible in the video frame panel\n",
        "session.view = uncertain_view.exists(\"molmo_events\")\n",
        "\n",
        "print(\"In the App, for each uncertain sample, look at:\")\n",
        "print(\"  Timeline: gpt_action_brief | qwen_events | molmo_events\")\n",
        "print(\"  Frames:   molmo_actor (keypoints showing where the actor is)\")\n",
        "print(\"  Text:     gpt_root_summary | qwen_desc_summary\")\n",
        "print()\n",
        "print(\"Cases to find:\")\n",
        "print(\"  ✓ Molmo2 agrees with one model — tiebreaker worked\")\n",
        "print(\"  ? All three diverge — the video is genuinely ambiguous\")\n",
        "print(\"  ✗ Low score was just a paraphrase — lexical metric was wrong\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 6: The Payoff\n",
        "\n",
        "Let's look at what we built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fiftyone import ViewField as F \n",
        "\n",
        "# High-confidence subset: both models agree — ready to use\n",
        "high_confidence = dataset.match(F(\"annotation_confidence\") >= 0.6)\n",
        "print(f\"High-confidence subset (score ≥ 0.6): {len(high_confidence)} samples\")\n",
        "\n",
        "# Review queue: models disagree — worth a closer look\n",
        "review_queue = dataset.match(F(\"annotation_confidence\") < 0.3)\n",
        "print(f\"Review queue (score < 0.3):           {len(review_queue)} samples\")\n",
        "\n",
        "# Open the full dataset with everything visible\n",
        "session.view = dataset\n",
        "print(\"\\nFull dataset loaded in App. Use Filter to slice by annotation_confidence.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What We Built\n",
        "\n",
        "We started with 1,144 videos and a set of annotations we had to take on faith.\n",
        "\n",
        "We ended with a **confidence map**:\n",
        "\n",
        "- **High confidence (`annotation_confidence ≥ 0.6`)** — both GPT-OSS-120B and Qwen3-VL describe these clips similarly. The annotation is likely reliable. Use these for training or evaluation.\n",
        "\n",
        "- **Low confidence (`annotation_confidence < 0.3`)** — the models diverge. Some of these are just paraphrase failures (a limitation of lexical metrics). Others are genuinely ambiguous clips where even multiple frontier models don't agree. These are your review queue — and Molmo2's spatial grounding is your best tool for investigating them.\n",
        "\n",
        "---\n",
        "\n",
        "### The Workflow, Generalized\n",
        "\n",
        "Everything we did here transfers directly to any video dataset with AI-generated labels:\n",
        "\n",
        "1. **Load and explore** — understand the annotation structure before trusting it\n",
        "2. **Embed from multiple angles** — visual content, visual grounding, language; each lens reveals something different\n",
        "3. **Generate an independent second opinion** — run a different model, compare outputs\n",
        "4. **Quantify agreement** — a score per sample turns qualitative inspection into a sortable, filterable signal\n",
        "5. **Ground the disagreements** — when text descriptions conflict, spatial evidence is the tiebreaker\n",
        "\n",
        "The tools: **FiftyOne** for orchestration and visualization, **Qwen3-VL-Embedding** for the searchable visual index, **Qwen3-VL** for independent annotation, **Molmo2** for grounding, and a **text evaluation plugin** to quantify it all.\n",
        "\n",
        "The dataset doesn't matter. The workflow does."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fo_video_workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
