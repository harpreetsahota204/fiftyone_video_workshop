{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/fiftyone_video_workshop/blob/main/workshop.ipynb)\n",
        "\n",
        "# Understanding Video Data at Scale\n",
        "### A Hands-On Workshop with Action100M and FiftyOne\n",
        "\n",
        "---\n",
        "\n",
        "Video is a hard modality to work with. \n",
        "\n",
        "You're dealing with more data, temporal complexity, and annotation workflows that don't scale. This notebook tackles a practical question: **given a large video dataset, how do you understand what's in it without manually watching thousands of clips?**\n",
        "\n",
        "We're working with a subset of **[Action100M preview](https://www.arxiv.org/abs/2601.10592)**. In this subset there are 1,144 YouTube videos, each clipped to 90 seconds, annotated with a hierarchical *Tree-of-Captions* structure produced by a fully automated AI pipeline: V-JEPA 2 for segmentation, PerceptionLM-3B and Llama-3.2-Vision-11B for captioning, and GPT-OSS-120B with multi-round Self-Refine for structured annotation extraction.\n",
        "\n",
        "Every label in this dataset was written by a model. None of it was seen by a human annotator.\n",
        "\n",
        "As AI-generated datasets become the norm, **the skill of interrogating machine-generated annotations is increasingly important**. This notebook shows you how to do that systematically.\n",
        "\n",
        "---\n",
        "\n",
        "### What We'll Build\n",
        "\n",
        "|  | Question | Tools |\n",
        "|---|---|---|\n",
        "| 1. What We Were Given | What does this dataset claim to contain? | FiftyOne App |\n",
        "| 2. Three Lenses | What does the raw data actually look like? | Qwen3-VL-Embedding, Molmo2, Sentence Transformers |\n",
        "| 3. The Second Opinion | Does a second model agree with the first? | Qwen3-VL |\n",
        "| 4. Measuring Agreement | How much do they agree, per sample? | Text Evaluation Plugin |\n",
        "| 5. Grounding the Hard Cases | Where they disagree, who's right? | Molmo2 |\n",
        "| 6. The Payoff | What can I now do with this? | FiftyOne App |\n",
        "\n",
        "By the end, you'll have a **confidence map** of the dataset's annotations and a reusable workflow for understanding any video dataset with AI-generated labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "fo.config.requirement_error_level=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 1: What We Were Given\n",
        "\n",
        "Before running any models, let's understand the shape of what we have. \n",
        "\n",
        "The Action100M preview comes with rich pre-existing annotations from the Tree-of-Captions pipeline. Each video has temporal segments annotated at multiple levels of granularity:\n",
        "\n",
        "- **Level 0 (root/tier):** The full video — one annotation covering the entire 90-second clip\n",
        "- **Mid levels:** Sub-segments — multi-second chunks describing coherent activities\n",
        "- **Leaf level:** The finest grain — individual moments\n",
        "\n",
        "At each level, there are five annotation fields:\n",
        "- `gpt_summary_brief` — one-sentence clip caption\n",
        "- `gpt_summary_detailed` — full play-by-play description\n",
        "- `gpt_action_brief` — short verb phrase (\"spread almonds on tray\")\n",
        "- `gpt_action_detailed` — instruction-manual version of the action\n",
        "- `gpt_action_actor` — who's performing it\n",
        "\n",
        "All of these were generated by GPT-OSS-120B with three rounds of Self-Refine. We're going to take these at face value for now — and then build the tools to interrogate them.\n",
        "\n",
        "You can either download the dataset from the [Voxel51 Hugging Face org](https://huggingface.co/datasets/Voxel51/action100m_tiny_subset), or if you face rate limit issues, you can follow the instructions in the `download_scripts` directory of this repository to download the dataset and parse it into FiftyOne format.\n",
        "\n",
        "If you are downloading from the Hugging Face Hub, run:\n",
        "\n",
        "```python\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\n",
        "    \"Voxel51/action100m_tiny_subset\",\n",
        "    dataset_name=\"action100m\",\n",
        "    overwrite=True,\n",
        "    persistent=True,\n",
        ")\n",
        "```\n",
        "\n",
        "If you wat to use the dataset with all the enrichments, then run:\n",
        "\n",
        "```python\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\n",
        "    \"harpreetsahota/fo_video_workshop_enriched\",\n",
        "    dataset_name=\"action100m_enriched\",\n",
        "    overwrite=True,\n",
        "    persistent=True,\n",
        ")\n",
        "```\n",
        "\n",
        "If you've downloaded the dataset and parsed manually, then run:\n",
        "\n",
        "```python\n",
        "dataset = fo.load_dataset(\"action100m\")\n",
        "```\n",
        "\n",
        "I will assume that you'll want the datasets with all the enrichments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\n",
        "    \"harpreetsahota/fo_video_workshop_enriched\",\n",
        "    dataset_name=\"action100m_enriched\",\n",
        "    overwrite=True,\n",
        "    persistent=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "471823d7",
      "metadata": {},
      "source": [
        "A **[FiftyOne dataset](https://docs.voxel51.com/user_guide/using_datasets.html)** is the core data structure you work with in [FiftyOne](https://docs.voxel51.com/user_guide/basics.html). It:\n",
        "\n",
        "- Logically represents your visual data (images, videos, point clouds, etc.) along with all associated information: labels, metadata, predictions, and other fields.\n",
        "\n",
        "- Is stored in a lightweight, non-relational database (MongoDB) so it can scale to large datasets without loading all media into RAM. \n",
        "\n",
        "- Can be created from many sources: directories of files, common dataset formats (like COCO), or the built‑in Dataset Zoo. \n",
        "\n",
        "Conceptually, you can [think of a dataset like a table in pandas](https://github.com/voxel51/fiftyone/blob/develop/docs/source/tutorials/pandas_comparison.ipynb): it has **rows and columns**, but specialized for computer vision:\n",
        "\n",
        "- In pandas: **row = record**, **column = feature**\n",
        "- In FiftyOne: **sample = record**, **field = feature** \n",
        "\n",
        "You can inspect the schema of a dataset by calling it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26754040",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7ab8fb",
      "metadata": {},
      "source": [
        "### What is a sample?\n",
        "\n",
        "A [**sample**](https://docs.voxel51.com/user_guide/basics.html#samples) is the atomic element of a FiftyOne dataset. It is:\n",
        "\n",
        "- One “item” of your data (for example, a single image or a single video) plus everything you know about it. \n",
        "- A flexible container of [**fields**](https://docs.voxel51.com/user_guide/basics.html#fields), which can include:\n",
        "  - `filepath` to the media\n",
        "  - `metadata` (dimensions, duration, etc.)\n",
        "  - Ground truth labels (detections, classifications, segmentations, etc.)\n",
        "  - Model predictions\n",
        "  - Tags, scalar values, strings, arrays, and more \n",
        "\n",
        "So, **a dataset is a collection of samples**, and **each sample is everything you care about for one media file**, stored in a structured way that’s easy to query, visualize, and modify.\n",
        "\n",
        "You can see what a sample looks like by inspecting the [first](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first) sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec0d626",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5436d44",
      "metadata": {},
      "source": [
        "Once you have your dataset, you can [launch the app](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app) and see what's in it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57635f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open the FiftyOne App and explore the dataset\n",
        "session = fo.launch_app(dataset, auto=False)\n",
        "session.url"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cad16bf7",
      "metadata": {},
      "source": [
        "**What to look at in the App:**\n",
        "\n",
        "1. Play a few videos and toggle on `gpt_action_brief` — watch how the temporal segments track the actions you see on screen\n",
        "2. Click on a sample with high `tree_depth` (try sorting by it) — notice how the annotations at level 0 are broad overviews, while leaf-level annotations are very fine-grained moments\n",
        "3. Look at the `transcript` field — this is the raw ASR text, much noisier than the GPT-refined annotations\n",
        "4. Check a few `gpt_summary_detailed` labels — these are the longest annotations (~540 words average)\n",
        "\n",
        "> **Key question to hold in mind:** Every one of these labels came from an automated pipeline. They look authoritative. But should we trust them?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0244b7",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 2: Three Lenses on the Same Data\n",
        "\n",
        "Before interrogating the existing annotations, let's build our own understanding of the dataset — using nothing but the raw videos and transcripts.\n",
        "\n",
        "We'll create three separate embedding spaces:\n",
        "\n",
        "1. **Visual ([Qwen3-VL-Embedding](https://docs.voxel51.com/plugins/plugins_ecosystem/qwen3vl_embeddings.html)):** What the videos look like — and crucially, this lives in a shared text-video space, so we can search with natural language\n",
        "\n",
        "2. **Visual-Grounding ([Molmo2](https://docs.voxel51.com/plugins/plugins_ecosystem/molmo2.html)):** A different visual understanding — video-to-video similarity only, but from a model trained on grounding and spatial reasoning\n",
        "\n",
        "3. **Language (Transcript):** What people are *saying* in these videos, embedded with a text model\n",
        "\n",
        "The insight from comparing these three spaces is the foundation of everything that follows: **what you embed determines what you find**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036f351b",
      "metadata": {},
      "source": [
        "### Lens 1: Visual Content (Qwen3-VL-Embedding)\n",
        "\n",
        "[Qwen3-VL-Embedding](https://huggingface.co/Qwen/Qwen3-VL-Embedding-2B) maps video and text into a shared vector space. This means we can embed a natural language query and find videos that match without touching any of the existing labels. It's semantic search, not keyword search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af062de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the 2B embedding model — good balance of quality and speed\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/qwen3vl_embeddings\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "qwen_emb_model = foz.load_zoo_model(\n",
        "    \"Qwen/Qwen3-VL-Embedding-2B\",\n",
        ")\n",
        "\n",
        "qwen_emb_model.max_length=32768\n",
        "\n",
        "# Compute embeddings — stores a vector on each sample\n",
        "dataset.compute_embeddings(\n",
        "    qwen_emb_model,\n",
        "    embeddings_field=\"qwen_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=8,\n",
        "    skip_failures=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6907630f",
      "metadata": {},
      "source": [
        "### What you can do after computing embeddings in FiftyOne\n",
        "\n",
        "Once you've computed embeddings, you unlock powerful workflows:\n",
        "\n",
        "#### Visualize your embeddings in the App\n",
        "\n",
        "[Use `compute_visualization()`](https://docs.voxel51.com/brain.html#visualizing-embeddings) to reduce embeddings with UMAP, t‑SNE, or PCA and explore them in the Embeddings panel to:\n",
        "- See clusters and data structure  \n",
        "- Understand how classes group together  \n",
        "- Lasso regions to create views and filter subsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33179678",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "# Project into 2D with UMAP for visualization\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"qwen_viz\",\n",
        "    embeddings=\"qwen_embeddings\",\n",
        "    num_dims=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db429e1d",
      "metadata": {},
      "source": [
        "\n",
        "####  Search by visual similarity\n",
        "\n",
        "[Build a similarity index](https://docs.voxel51.com/brain.html#similarity) with `compute_similarity()` to:\n",
        "- Find similar images (image‑to‑image search)  \n",
        "- Detect duplicates or unique samples  \n",
        "- Enable visual search in the App or via Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a14462",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a similarity index — this is what powers text-to-video search\n",
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    brain_key=\"qwen_sim\",\n",
        "    embeddings=\"qwen_embeddings\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "967bae59",
      "metadata": {},
      "source": [
        "You can also use this model for zero-shot classification. Let's add a Field to the Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ebd6a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = [\n",
        "    \"Cooking and Food\",\n",
        "    \"Home Improvement and DIY\",\n",
        "    \"Health and Beauty\",\n",
        "    \"Hobbies and Crafts\",\n",
        "    \"Sports and Fitness\",\n",
        "    \"Gardening\",\n",
        "    \"Technology and Electronics\",\n",
        "    \"Fashion and Style\",\n",
        "    \"Arts and Music\",\n",
        "    \"Automotive\",\n",
        "    \"Pets and Animals\",\n",
        "    \"Education and Learning\"\n",
        "]\n",
        "\n",
        "# Configure model for classification\n",
        "qwen_emb_model.classes = classes\n",
        "qwen_emb_model.text_prompt = \"A video about \"\n",
        "\n",
        "# Apply zero-shot classification\n",
        "dataset.apply_model(\n",
        "    qwen_emb_model, \n",
        "    label_field=\"predicted_class\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lens 2: Visual-Grounding (Molmo2)\n",
        "\n",
        "Molmo2 is a different kind of vision-language model — it's trained heavily on grounding tasks: pointing to objects, tracking them through time, and counting. Its internal representations will weight spatial and motion features differently than Qwen3-VL-Embedding.\n",
        "\n",
        "We can't do text-to-video search with Molmo2 embeddings, but we can still visualize the embedding space and do video-to-video similarity. \n",
        "\n",
        "The question is: **do two different visual models agree on which videos are similar?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz \n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/molmo2\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "molmo_model = foz.load_zoo_model(\"allenai/Molmo2-4B\")\n",
        "\n",
        "molmo_model.pooling_strategy = \"mean\"\n",
        "\n",
        "dataset.compute_embeddings(\n",
        "    molmo_model,\n",
        "    embeddings_field=\"molmo_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=4,\n",
        "    skip_failures=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"molmo_viz\",\n",
        "    embeddings=\"molmo_embeddings\",\n",
        "    num_dims=2,\n",
        ")\n",
        "\n",
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    brain_key=\"molmo_sim\",\n",
        "    embeddings=\"molmo_embeddings\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lens 3: Language (Transcript Embeddings)\n",
        "\n",
        "Both visual embeddings above represent what you *see* in the videos. Now let's embed what you *hear* — the ASR transcript text.\n",
        "\n",
        "Instructional videos vary enormously in how much they narrate. Some presenters talk through every step; others work in silence. The transcript embedding space captures this variation in a way neither visual model can.\n",
        "\n",
        "Fo this we can make use of [`jina-embeddings-v5-text-small-clustering`](https://huggingface.co/jinaai/jina-embeddings-v5-text-small-clustering). \n",
        "\n",
        "This model is not integrated as a remote source zoo model, but we can make use of it by extracting the transcripts from the videos using the `values` method of the [Dataset](https://docs.voxel51.com/user_guide/basics.html#datasets) to get all the values of the [Field](https://docs.voxel51.com/user_guide/basics.html#fields) into a Python list.\n",
        "\n",
        "With the values in a list we can use the model natively and then add the results back as a Field to the Dataset using [the `set_values`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.set_values) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#set an environment variable so tokenizers doesn't yell at us,\n",
        "# note this related to the `transformers` and `tokenizers` libraries and not a FiftyOne specific environment variable\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "text_emb_model = SentenceTransformer(\n",
        "    \"jinaai/jina-embeddings-v5-text-small-clustering\",\n",
        "    model_kwargs={\"dtype\": torch.bfloat16}, #recommended for GPUs\n",
        "    config_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device\":\"cuda\"}, #flash attn optional, but recommended\n",
        "    tokenizer_kwargs={\"extra_special_tokens\": {}}, # This line fixes the AttributeError: 'list' object has no attribute 'keys'\n",
        ")\n",
        "\n",
        "transcripts = dataset.values(\"transcript\")\n",
        "\n",
        "# Encode texts\n",
        "text_embeddings = text_emb_model.encode(transcripts)\n",
        "\n",
        "dataset.set_values(\"text_embeddings\", text_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e64caaf",
      "metadata": {},
      "source": [
        "We can compute visualizations of the embeddings just like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f245ca5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob \n",
        "\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"transcript_viz\",\n",
        "    embeddings=\"text_embeddings\",\n",
        "    num_dims=2,\n",
        ")\n",
        "\n",
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    brain_key=\"text_sim\",\n",
        "    embeddings=\"text_embeddings\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6c5c8e",
      "metadata": {},
      "source": [
        "We can also generate some Classifications for the Samples based on the title and description:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28194845",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_cls_model = SentenceTransformer(\n",
        "    \"jinaai/jina-embeddings-v5-text-small-classification\",\n",
        "    model_kwargs={\"dtype\": torch.bfloat16}, #recommended for GPUs\n",
        "    config_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device\":\"cuda\"}, #flash attn optional, but recommended\n",
        "    tokenizer_kwargs={\"extra_special_tokens\": {}}, # This line fixes the AttributeError: 'list' object has no attribute 'keys'\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# grab the titles for the videos\n",
        "_titles = dataset.values(\"title\")\n",
        "\n",
        "# grab the descriptions for the videos\n",
        "_desc = dataset.values(\"description\")\n",
        "\n",
        "# combine them into one string\n",
        "title_desc = [t + \" \" + d for t, d in zip(_titles, _desc)]\n",
        "\n",
        "# Encode texts\n",
        "title_embeddings = text_cls_model.encode(title_desc, prompt_name=\"document\")\n",
        "\n",
        "class_embeddings = text_cls_model.encode(classes, prompt = \"The title of a YouTube video about \")\n",
        "\n",
        "# compute cosine similarity\n",
        "similarity = text_cls_model.similarity(title_embeddings, class_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7298af6",
      "metadata": {},
      "source": [
        "Now let's parse this back `similarity`, which is a [num_samples × num_classes] tensor of cosine similarity scores.\n",
        "\n",
        "Each row is one video, each column is one class, and each value measures how geometrically close the text embedding is to that class label's embedding.\n",
        "\n",
        "`argmax` gives us the column index (class) with the highest score per row (sample), so we can slice into our classes list, and `max` gives us the score itself, which we store as confidence.\n",
        "\n",
        "This is zero-shot classification: no fine-tuning, no labeled training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17d9c61f",
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_indices = similarity.argmax(dim=1).tolist()\n",
        "\n",
        "confidence_scores = similarity.max(dim=1).values.tolist()\n",
        "\n",
        "predicted_labels = [\n",
        "    fo.Classification(\n",
        "        label=classes[idx],\n",
        "        confidence=conf,\n",
        "    )\n",
        "    for i, (idx, conf) in enumerate(zip(predicted_indices, confidence_scores))\n",
        "]\n",
        "\n",
        "dataset.set_values(\"jina_predicted_class\", predicted_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57fbfc5d",
      "metadata": {},
      "source": [
        "Now, we don't have ground truth classifications for the classification of the videos. \n",
        "\n",
        "So, let's assume that the classifications we got when using Qwen3-VL-Embeddings are the ground truth. \n",
        "\n",
        "We can evaluate how well the classifications using the Jina text embedding model is like so: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab7af44",
      "metadata": {},
      "outputs": [],
      "source": [
        "classification_results = dataset.evaluate_classifications(\n",
        "    \"jina_predicted_class\",\n",
        "    gt_field=\"predicted_class\",\n",
        "    eval_key=\"simple_cls_eval\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ded186dd",
      "metadata": {},
      "source": [
        "I'll show you a more interactive evaluation panel when we return to the app, but for now you can view the results at a high-level like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0afe041e",
      "metadata": {},
      "outputs": [],
      "source": [
        "classification_results.print_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6cb543b",
      "metadata": {},
      "source": [
        "### Using Embeddings to Compute Uniqueness and Representativeness Values\n",
        "\n",
        "In FiftyOne, both **uniqueness** and **representativeness** are scalar scores computed from embeddings that describe how a sample relates to the rest of the dataset.\n",
        "\n",
        "#### Uniqueness\n",
        "\n",
        "- [Measures **how different (dissimilar)**](https://docs.voxel51.com/brain.html#image-uniqueness) a sample is from its neighbors in embedding space.  \n",
        "\n",
        "- Implemented by looking at each sample’s nearest neighbors, weighting their distances (e.g. 60%-30%-10%), and normalizing to a score in **\\[0, 1\\]**. Higher scores mean the sample is more “isolated” or distinct; lower scores mean it has many close neighbors.\n",
        " \n",
        "- The most unique sample in a collection has a uniqueness value of **1**.\n",
        "\n",
        "- Useful for:\n",
        "  - Finding **outliers / edge cases / bad actors**  \n",
        "  - Detecting **near-duplicates** (by looking at *low* uniqueness)  \n",
        "  - Selecting **diverse samples** for annotation when budget is limited \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ae91c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_uniqueness(\n",
        "    dataset,\n",
        "    uniqueness_field = \"qwen_uniqueness\",\n",
        "    embeddings = \"qwen_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fcd344b",
      "metadata": {},
      "source": [
        "## Representativeness\n",
        "\n",
        "- [Measures **how typical** or **central** ](https://docs.voxel51.com/brain.html#image-representativeness) a sample is relative to the rest of the dataset in embedding space.  \n",
        "\n",
        "- Also normalized to **\\[0, 1\\]**, with the most representative samples having value **1**.  \n",
        "\n",
        "- High representativeness ≈ sample lies in a dense, central region of the data; it’s similar to many other samples.  \n",
        "\n",
        "- Useful for:\n",
        "  - **Active learning**: picking representative samples to cover common patterns  \n",
        "  - **Dataset balancing**: finding under/overrepresented regions  \n",
        "  - **Efficient annotation**: prioritizing samples that “stand in” for many others \n",
        "\n",
        "In short:\n",
        "\n",
        "- **Uniqueness** → “How unusual is this sample?”  \n",
        "- **Representativeness** → “How well does this sample represent many others?”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad16d93",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_representativeness(\n",
        "    dataset,\n",
        "    representativeness_field = \"qwen_rep\",\n",
        "    embeddings = \"qwen_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Cross-Lens Moment\n",
        "\n",
        "Try to **find a tight visual cluster in the Qwen UMAP, then look at where those same videos land in the transcript UMAP.**\n",
        "\n",
        "You may find that videos which cluster tightly by visual content are often spread out in transcript space. Cooking videos that all look similar — same kitchen setup, same hands-on-food visual — may use completely different spoken language: detailed step-by-step narration, casual conversational chat, or total silence.\n",
        "\n",
        "This is not a failure of any embedding model, but a property of the data: **visual similarity and linguistic similarity are measuring different things.** Your choice of embedding determines what questions you can ask.\n",
        "\n",
        "- Qwen visual embeddings to find *visually similar content* or do text-to-video semantic search\n",
        "\n",
        "- Molmo2 embeddings to find videos with similar *physical actions and spatial structure*\n",
        "\n",
        "- Transcript embeddings to find videos that *talk about similar topics*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick a video and find its nearest neighbors in each embedding space — \n",
        "# a concrete way to see how the three spaces differ\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "sample = dataset.first()\n",
        "\n",
        "root_view = dataset.filter_labels(\"gpt_summary_brief\", F(\"tier\") == \"root\")\n",
        "\n",
        "print(f\"Reference video: {sample.title}\")\n",
        "ref_root = root_view[sample.id].gpt_summary_brief.detections[0].label\n",
        "print(f\"Root summary: {ref_root}\\n\")\n",
        "\n",
        "for brain_key, label in [\n",
        "    (\"qwen_sim\", \"Based on QwenVL Embeddings\"), \n",
        "    (\"molmo_sim\", \"Based on Molmo Embeddings\"), \n",
        "    (\"text_sim\", \"Based on Text Embeddings\")\n",
        "    ]:\n",
        "    neighbors = root_view.sort_by_similarity(sample.id, brain_key=brain_key, k=4)\n",
        "    print(f\"Nearest neighbors ({label}):\")\n",
        "    for n in neighbors.skip(1).take(3):\n",
        "        print(f\"  → {n.gpt_summary_brief.detections[0].label}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e305612",
      "metadata": {},
      "source": [
        "**What to look at:** \n",
        "\n",
        "- In the Embeddings panel, hover over clusters. Click points to see which videos land near each other. Try coloring by `tree_depth` or `title` to see if the visual clusters map onto any metadata patterns.\n",
        "\n",
        "\n",
        "- Compare the Molmo2 UMAP with the Qwen UMAP. Are the clusters in the same positions? Similar shape, different boundaries — this is two models with different visual priors producing different embedding spaces. Neither is \"correct\" — they're measuring different aspects of the same content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 3: The Second Opinion\n",
        "\n",
        "We've built three independent maps of the dataset. Now let's do what the original pipeline did — but with a completely different model.\n",
        "\n",
        "The Action100M annotations were produced by GPT-OSS-120B processing outputs from PerceptionLM-3B and Llama-3.2-Vision-11B. \n",
        "\n",
        "We're going to run **Qwen3-VL-8B** on the same videos, completely independently, and ask:\n",
        "\n",
        "- How would *you* describe this video?\n",
        "\n",
        "- What events do *you* see, and when?\n",
        "\n",
        "We're not trying to beat the pipeline. We're trying to get a second opinion. Where the two agree, we gain confidence in the original annotation. Where they diverge, we've found something worth looking at."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz \n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/qwen3vl_video\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "qwen_video_model = foz.load_zoo_model(\n",
        "    \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b61fd92",
      "metadata": {},
      "source": [
        "#### Generate detailed video description\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a full-video description for every sample.\n",
        "# This produces a string field: qwen_desc_summary\n",
        "qwen_video_model.operation = \"description\"\n",
        "\n",
        "dataset.apply_model(\n",
        "    qwen_video_model,\n",
        "    label_field=\"qwen_desc\",\n",
        "    skip_failures=True,\n",
        ")\n",
        "\n",
        "print(f\"Descriptions generated: {dataset.exists('qwen_desc_summary').count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e915724",
      "metadata": {},
      "source": [
        "#### Comprehensive analysis\n",
        "\n",
        "Analyzes video for all aspects: description, events, objects, scene info, activities.\n",
        "\n",
        "Output fields:\n",
        "\n",
        "- `analysis_summary` - Video description (string)\n",
        "- `analysis_events` - Temporal events (fo.TemporalDetections)\n",
        "- `analysis_objects` - Object appearances (fo.TemporalDetections)\n",
        "- `analysis_scene_info_*` - Scene classifications\n",
        "- `analysis_activities_*` - Activity classifications\n",
        "- `sample.frames[N].objects` - Frame-level object detections\n",
        "- `sample.frames[N].text_content` - Frame-level OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qwen_video_model.operation = \"comprehensive\"\n",
        "\n",
        "dataset.apply_model(\n",
        "    qwen_video_model,\n",
        "    label_field=\"qwen_comp\",\n",
        "    skip_failures=True,\n",
        ")\n",
        "\n",
        "print(f\"Temporal events generated: {dataset.exists('qwen_events').count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice the descriptions often describe the same video in quite different language — different vocabulary, different sentence structure, different level of detail. Sometimes one catches a detail the other misses. \n",
        "\n",
        "The question is: **how do we move from \"these look different\" to a number we can sort and filter by?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fecac697",
      "metadata": {},
      "source": [
        "#### Using a prompt from the paper\n",
        "\n",
        "In the paper they describe some prompts they use. We can prompt Qwen3-VL the same way and see what we end up with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e24e62",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT = \"\"\"Identify the main actor and the physical action performed in the current segment. Provide both a brief\n",
        "description that represents the overall action step, and a detailed description that contains sufficient\n",
        "procedural detail. Use \"N/A\" (without further explaination) if there are no visible actors or physical\n",
        "actions (e.g., static).\n",
        "\n",
        "# Response Formats\n",
        "## output\n",
        "{\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"summary\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"brief\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Single sentence video caption.\"\n",
        "                },\n",
        "                \"detailed\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Detailed, comprehensive description.\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"action\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"brief\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"A single verb phrase (no -ing forms) brifly summarizing the overall action content.\"\n",
        "                },\n",
        "                \"detailed\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"A single imperitive sentence describing how the action is performed with more details.\"\n",
        "                },\n",
        "                \"actor\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Single sentece or an imformative noun phrase describing who is performing the action.\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"summary\", \"action\"]\n",
        "}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a5e7a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a full-video description for every sample.\n",
        "# This produces a string field: qwen_desc_summary\n",
        "\n",
        "\n",
        "qwen_video_model.operation = \"custom\"\n",
        "\n",
        "qwen_video_model.prompt = PROMPT\n",
        "\n",
        "dataset.apply_model(\n",
        "    qwen_video_model,\n",
        "    label_field=\"qwen_custom\",\n",
        "    skip_failures=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e5d749",
      "metadata": {},
      "source": [
        "We can also do the same with the Molmo2 model, let's use the 8B parameter model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f3fd32",
      "metadata": {},
      "outputs": [],
      "source": [
        "molmo_8b_model = foz.load_zoo_model(\"allenai/Molmo2-8B\")\n",
        "\n",
        "molmo_8b_model.operation = \"describe\"\n",
        "\n",
        "molmo_8b_model.prompt = PROMPT\n",
        "\n",
        "dataset.apply_model(\n",
        "    molmo_8b_model,\n",
        "    label_field=\"molmo_custom\",\n",
        "    skip_failures=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "128fc6d1",
      "metadata": {},
      "source": [
        "The model returned its output as a raw JSON string stored in `qwen_custom_result`.\n",
        "\n",
        "We parse the JSON once and promote each nested value to its own top-level string field. \n",
        "\n",
        "This makes every piece of structured output a first-class citizen in the dataset: filterable in the App, embeddable with a text model, and comparable against the existing GPT-generated fields (`gpt_summary_brief`, `gpt_action_actor`, etc.) that ship with the Action100M dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ab545e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def extract(raw):\n",
        "    try:\n",
        "        parsed = json.loads(raw)\n",
        "        props = parsed[\"properties\"]\n",
        "        s = props[\"summary\"][\"properties\"]\n",
        "        a = props[\"action\"][\"properties\"]\n",
        "        return s.get(\"brief\"), s.get(\"detailed\"), a.get(\"brief\"), a.get(\"detailed\"), a.get(\"actor\")\n",
        "    except:\n",
        "        return None, None, None, None, None\n",
        "\n",
        "SOURCES = [\n",
        "    (\"qwen_custom_result\",       \"qwen3vl\"),\n",
        "    (\"molmo_custom_description\", \"molmo\"),\n",
        "]\n",
        "\n",
        "for src_field, prefix in SOURCES:\n",
        "    raws = dataset.values(src_field)\n",
        "    results = [extract(r) for r in raws]\n",
        "\n",
        "    dataset.set_values(f\"{prefix}_summary_brief\",    [r[0] for r in results])\n",
        "    dataset.set_values(f\"{prefix}_summary_detailed\", [r[1] for r in results])\n",
        "    dataset.set_values(f\"{prefix}_action_brief\",     [r[2] for r in results])\n",
        "    dataset.set_values(f\"{prefix}_action_detailed\",  [r[3] for r in results])\n",
        "    dataset.set_values(f\"{prefix}_action_actor\",     [r[4] for r in results])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1800e15",
      "metadata": {},
      "source": [
        "Each `gpt_summary_brief`, `gpt_summary_detailed`, etc. is a [`TemporalDetections` field](https://docs.voxel51.com/user_guide/using_datasets.html#temporal-detection) containing a list of detections across multiple tiers. \n",
        "\n",
        "We want to \"hoist\" just the root-tier detection's label string up to a flat sample-level field for easy filtering and comparison. To accomplish this we can [use a combination of `ViewField` and `Filtering`](https://docs.voxel51.com/user_guide/using_views.html#querying-samples).\n",
        "\n",
        "This is necessary for the next step in the workshop, where we will compare the Qwen3-VL outputs with the \"ground truth\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b489b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "# Fields to extract root labels from, and their new target field names\n",
        "GPT_FIELDS = {\n",
        "    \"gpt_summary_brief\":    \"gpt_summary_root_brief\",\n",
        "    \"gpt_summary_detailed\": \"gpt_summary_root_detailed\",\n",
        "    \"gpt_action_brief\":     \"gpt_action_root_brief\",\n",
        "    \"gpt_action_detailed\":  \"gpt_action_root_detailed\",\n",
        "    \"gpt_action_actor\":     \"gpt_action_root_actor\",\n",
        "}\n",
        "\n",
        "for src_field, dst_field in GPT_FIELDS.items():\n",
        "    # Create a view that filters each sample's detections to root-tier only.\n",
        "    # This does NOT mutate the dataset — it's a virtual filter.\n",
        "    root_view = dataset.filter_labels(src_field, F(\"tier\") == \"root\")\n",
        "\n",
        "    # values() on a TemporalDetections field returns a list-of-lists:\n",
        "    # one inner list per sample, containing the labels of surviving detections.\n",
        "    # Since there is exactly one root per sample, each inner list has one element.\n",
        "    nested = root_view.values(f\"{src_field}.detections.label\")\n",
        "\n",
        "    # Flatten: take the first (only) label, or None if a sample had no root.\n",
        "    flat = [labels[0] if labels else None for labels in nested]\n",
        "\n",
        "    dataset.set_values(dst_field, flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 4: Measuring Agreement\n",
        "\n",
        "Eyeballing agreement doesn't scale. We need metrics.\n",
        "\n",
        "\n",
        "##### **Normalized Levenshtein Similarity** \n",
        "\n",
        "This metric measures how similar two text strings are on a scale from 0.0 to 1.0.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **Levenshtein Distance**: Counts the minimum number of single-character edits (insertions, deletions, substitutions) needed to transform one string into another.\n",
        "\n",
        "2. **Normalization**: Converts the distance to a similarity score:\n",
        "   \n",
        "$$\n",
        "\\text{similarity} = 1.0 - \\frac{\\text{Levenshtein distance}}{\\max(\\text{len}(s_1), \\text{len}(s_2))}\n",
        "$$\n",
        "\n",
        "\n",
        "3. **Score**: \n",
        "   - **1.0** = perfect match\n",
        "   - **0.0** = completely different\n",
        "   - **0.0-1.0** = partial similarity\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **\"hello\"** vs **\"hello\"** → 1.0 (no edits)\n",
        "- **\"hello\"** vs **\"helo\"** → 0.8 (1 deletion / 5 chars)\n",
        "- **\"hello\"** vs **\"world\"** → 0.2 (4 substitutions / 5 chars)\n",
        "\n",
        "**Use Cases:**\n",
        "\n",
        "Evaluating OCR accuracy, transcription quality, text generation models, or any scenario requiring quantification of text similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.operators as foo\n",
        "\n",
        "sim_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_normalized_similarity\")\n",
        "\n",
        "# (pred_field, gt_field, output_field)\n",
        "# lex_q_ = lexical similarity, Qwen vs GPT root\n",
        "# lex_m_ = lexical similarity, Molmo vs GPT root\n",
        "COMPARISON_PAIRS = [\n",
        "    (\"qwen3vl_summary_brief\",    \"gpt_summary_root_brief\",    \"lex_q_sum_brief\"),\n",
        "    (\"qwen3vl_summary_detailed\", \"gpt_summary_root_detailed\", \"lex_q_sum_detailed\"),\n",
        "    (\"qwen3vl_action_brief\",     \"gpt_action_root_brief\",     \"lex_q_act_brief\"),\n",
        "    (\"qwen3vl_action_detailed\",  \"gpt_action_root_detailed\",  \"lex_q_act_detailed\"),\n",
        "    (\"qwen3vl_action_actor\",     \"gpt_action_root_actor\",     \"lex_q_actor\"),\n",
        "    (\"molmo_summary_brief\",      \"gpt_summary_root_brief\",    \"lex_m_sum_brief\"),\n",
        "    (\"molmo_summary_detailed\",   \"gpt_summary_root_detailed\", \"lex_m_sum_detailed\"),\n",
        "    (\"molmo_action_brief\",       \"gpt_action_root_brief\",     \"lex_m_act_brief\"),\n",
        "    (\"molmo_action_detailed\",    \"gpt_action_root_detailed\",  \"lex_m_act_detailed\"),\n",
        "    (\"molmo_action_actor\",       \"gpt_action_root_actor\",     \"lex_m_actor\"),\n",
        "]\n",
        "\n",
        "for pred_field, gt_field, output_field in COMPARISON_PAIRS:\n",
        "    sim_op(\n",
        "        dataset,\n",
        "        pred_field=pred_field,\n",
        "        gt_field=gt_field,\n",
        "        output_field=output_field,\n",
        "        case_sensitive=False,\n",
        "        delegate=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48fadb2a",
      "metadata": {},
      "source": [
        "This metric computes **Semantic Similarity** using neural embeddings, measuring whether two texts have the same *meaning* rather than the same *characters*.\n",
        "\n",
        "\n",
        "1. **Sentence Encoding**: Uses a pre-trained neural network (from `sentence-transformer`) to convert each text into a high-dimensional vector (embedding) that captures its meaning.\n",
        "\n",
        "2. **Cosine Similarity**: Computes the angle between the two embedding vectors:\n",
        "   \n",
        "   $$\n",
        "   \\text{similarity} = \\max(0, \\cos(\\text{embedding}_{\\text{gt}}, \\text{embedding}_{\\text{pred}}))\n",
        "   $$\n",
        "\n",
        "3. **Score**: \n",
        "   - **1.0** = semantically identical\n",
        "   - **0.0** = completely unrelated\n",
        "   - **0.0-1.0** = partial semantic overlap\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **\"The car is fast\"** vs **\"The automobile is quick\"** → ~0.95 (same meaning, different words)\n",
        "- **\"hello\"** vs **\"helo\"** → ~0.85 (similar meaning despite typo)\n",
        "- **\"cat\"** vs **\"dog\"** → ~0.6 (related concepts)\n",
        "\n",
        "**Comparison to Normalized Levenshtein Similarity:**\n",
        "\n",
        "**Levenshtein** measures character-level edits using string matching. It's very fast but penalizes any character difference, even if the meaning is preserved. Best for detecting typos, OCR errors, or when exact wording matters.\n",
        "\n",
        "**Semantic** measures meaning using neural embeddings. It's slower (requires model inference) but rewards conceptual equivalence regardless of wording. For example, \"car\" vs \"automobile\" scores 0.0 with Levenshtein but ~0.9 with Semantic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f74622",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.operators as foo\n",
        "\n",
        "sem_op = foo.get_operator(\n",
        "    \"@harpreetsahota/text-evaluation-metrics/compute_semantic_similarity\"\n",
        ")\n",
        "\n",
        "# (pred_field, gt_field, output_field)\n",
        "# sem_q_ = semantic similarity, Qwen vs GPT root\n",
        "# sem_m_ = semantic similarity, Molmo vs GPT root\n",
        "COMPARISON_PAIRS = [\n",
        "    (\"qwen3vl_summary_brief\",    \"gpt_summary_root_brief\",    \"sem_q_sum_brief\"),\n",
        "    (\"qwen3vl_summary_detailed\", \"gpt_summary_root_detailed\", \"sem_q_sum_detailed\"),\n",
        "    (\"qwen3vl_action_brief\",     \"gpt_action_root_brief\",     \"sem_q_act_brief\"),\n",
        "    (\"qwen3vl_action_detailed\",  \"gpt_action_root_detailed\",  \"sem_q_act_detailed\"),\n",
        "    (\"qwen3vl_action_actor\",     \"gpt_action_root_actor\",     \"sem_q_actor\"),\n",
        "    (\"molmo_summary_brief\",      \"gpt_summary_root_brief\",    \"sem_m_sum_brief\"),\n",
        "    (\"molmo_summary_detailed\",   \"gpt_summary_root_detailed\", \"sem_m_sum_detailed\"),\n",
        "    (\"molmo_action_brief\",       \"gpt_action_root_brief\",     \"sem_m_act_brief\"),\n",
        "    (\"molmo_action_detailed\",    \"gpt_action_root_detailed\",  \"sem_m_act_detailed\"),\n",
        "    (\"molmo_action_actor\",       \"gpt_action_root_actor\",     \"sem_m_actor\"),\n",
        "]\n",
        "\n",
        "for pred_field, gt_field, output_field in COMPARISON_PAIRS:\n",
        "    sem_op(\n",
        "        dataset,\n",
        "        pred_field=pred_field,\n",
        "        gt_field=gt_field,\n",
        "        model_name=\"all-mpnet-base-v2\",\n",
        "        output_field=output_field,\n",
        "        delegate=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What to look at:** Browse the bottom of the sorted view (lowest confidence). Are these actually bad annotations, or just paraphrases that look different? This is the limitation of lexical metrics — they can't tell the difference. A future extension here is a semantic similarity operator using sentence embeddings, which would score paraphrases much more accurately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "670dc414",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 5\n",
        "\n",
        "Now that we have both lexical and semantic agreement scores for every sample, we can slice the dataset into meaningful subsets — not just \"good\" and \"bad\", but a more nuanced three-way split that reveals something important about the limits of lexical evaluation alone.\n",
        "\n",
        "We'll demonstrate this using `summary_detailed` as our primary signal: it's the richest description field, so agreement or disagreement here is the most informative. The same pattern applies directly to any of the other four field pairs (`summary_brief`, `action_brief`, `action_detailed`, `action_actor`) — try them on your own and see if the distributions differ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "HIGH_LEX = 0.6\n",
        "HIGH_SEM = 0.6\n",
        "LOW_LEX  = 0.3\n",
        "LOW_SEM  = 0.5\n",
        "\n",
        "lex_q = F(\"lex_q_sum_detailed\")\n",
        "sem_q = F(\"sem_q_sum_detailed\")\n",
        "lex_m = F(\"lex_m_sum_detailed\")\n",
        "sem_m = F(\"sem_m_sum_detailed\")\n",
        "\n",
        "TAGS = {\n",
        "    \"qwen_agrees\":        (sem_q >= HIGH_SEM) & (lex_q >= HIGH_LEX),\n",
        "    \"molmo_agrees\":       (sem_m >= HIGH_SEM) & (lex_m >= HIGH_LEX),\n",
        "    \"triple_agreement\":   (sem_q >= HIGH_SEM) & (sem_m >= HIGH_SEM),\n",
        "    \"triple_disagreement\":(sem_q < LOW_SEM)   & (sem_m < LOW_SEM),\n",
        "    \"qwen_only\":          (sem_q >= HIGH_SEM) & (sem_m < LOW_SEM),\n",
        "    \"molmo_only\":         (sem_m >= HIGH_SEM) & (sem_q < LOW_SEM),\n",
        "    \"paraphrase\":         (lex_q < LOW_LEX)   & (sem_q >= HIGH_SEM) &\n",
        "                          (lex_m < LOW_LEX)   & (sem_m >= HIGH_SEM),\n",
        "}\n",
        "\n",
        "for tag, expr in TAGS.items():\n",
        "    view = dataset.match(expr)\n",
        "    view.tag_samples(tag)\n",
        "    print(f\"{tag:<25} → {len(view)} samples tagged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What We Built\n",
        "\n",
        "We started with 1,144 videos and a set of annotations we had to take on faith. Every label was written by a model. No human ever watched these clips.\n",
        "\n",
        "That's increasingly the norm. The question isn't whether to use AI-generated annotations but if you can blindly trust them, and for which samples.\n",
        "\n",
        "Here's what we built to answer that:\n",
        "\n",
        "- **Three independent views of the same data.** Qwen3-VL-Embedding, Molmo2, and transcript embeddings each carve the dataset differently: visual semantics, spatial grounding, and spoken language. Where they agree on which videos are similar, that structure is real. Where they diverge, you've learned something about the data that no single model would have told you.\n",
        "\n",
        "- **An independent second opinion on the annotations.** Qwen3-VL and Molmo2 watched the same videos cold, with no knowledge of what GPT-OSS-120B wrote. Where all three models converge, confidence is high. Where they diverge and especially where one model agrees with GPT and the other doesn't. By doing this, you've found the hard cases worth a human look.\n",
        "\n",
        "- **A per-sample confidence map.** Lexical similarity tells you whether the words match. Semantic similarity tells you whether the *meaning* matches. The gap between them (samples with low lexical but high semantic scores) is the **paraphrase class**: descriptions that look like disagreements but aren't. Without both metrics, you'd be reviewing the wrong samples.\n",
        "\n",
        "**Tags that survive.** Every insight is written back to the dataset as a sample tag — `triple_agreement`, `triple_disagreement`, `qwen_only`, `molmo_only`, `paraphrase`. Open the App, filter by tag, and the confidence map is right there.\n",
        "\n",
        "---\n",
        "\n",
        "### The Workflow, Generalized\n",
        "\n",
        "The dataset doesn't matter. This workflow does — and it transfers directly to any video dataset with AI-generated labels:\n",
        "\n",
        "1. **Explore before trusting** — understand the annotation structure, know what the pipeline actually produced\n",
        "\n",
        "2. **Embed from multiple angles** — visual content, spatial grounding, language; each lens surfaces different structure\n",
        "\n",
        "3. **Get a second opinion** — run an independent model, compare outputs field by field\n",
        "\n",
        "4. **Quantify agreement** — a per-sample score turns qualitative inspection into a sortable, filterable, taggable signal\n",
        "\n",
        "The uncomfortable truth about AI-generated datasets is that they look authoritative. The labels are structured, consistent, and comprehensive. But consistency isn't correctness. The only way to know which annotations to trust is to ask another model and measure how much they agree.\n",
        "\n",
        "That's what you just did."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fo_video_workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
