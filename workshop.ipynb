{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/fiftyone_video_workshop/blob/main/workshop.ipynb)\n",
        "\n",
        "# Understanding Video Data at Scale\n",
        "### A Hands-On Workshop with Action100M and FiftyOne\n",
        "\n",
        "---\n",
        "\n",
        "Video is a hard modality to work with. \n",
        "\n",
        "You're dealing with more data, temporal complexity, and annotation workflows that don't scale. This notebook tackles a practical question: **given a large video dataset, how do you understand what's in it without manually watching thousands of clips?**\n",
        "\n",
        "We're working with a subset of **[Action100M preview](https://www.arxiv.org/abs/2601.10592)**. In this subset there are 1,144 YouTube videos, each clipped to 90 seconds, annotated with a hierarchical *Tree-of-Captions* structure produced by a fully automated AI pipeline: V-JEPA 2 for segmentation, PerceptionLM-3B and Llama-3.2-Vision-11B for captioning, and GPT-OSS-120B with multi-round Self-Refine for structured annotation extraction.\n",
        "\n",
        "Every label in this dataset was written by a model. None of it was seen by a human annotator.\n",
        "\n",
        "As AI-generated datasets become the norm, **the skill of interrogating machine-generated annotations is increasingly important**. This notebook shows you how to do that systematically.\n",
        "\n",
        "---\n",
        "\n",
        "### What We'll Build\n",
        "\n",
        "|  | Question | Tools |\n",
        "|---|---|---|\n",
        "| 1. What We Were Given | What does this dataset claim to contain? | FiftyOne App |\n",
        "| 2. Three Lenses | What does the raw data actually look like? | Qwen3-VL-Embedding, Molmo2, Sentence Transformers |\n",
        "| 3. The Second Opinion | Does a second model agree with the first? | Qwen3-VL |\n",
        "| 4. Measuring Agreement | How much do they agree, per sample? | Text Evaluation Plugin |\n",
        "| 5. Grounding the Hard Cases | Where they disagree, who's right? | Molmo2 |\n",
        "| 6. The Payoff | What can I now do with this? | FiftyOne App |\n",
        "\n",
        "By the end, you'll have a **confidence map** of the dataset's annotations and a reusable workflow for understanding any video dataset with AI-generated labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "fo.config.requirement_error_level=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 1: What We Were Given\n",
        "\n",
        "Before running any models, let's understand the shape of what we have. \n",
        "\n",
        "The Action100M preview comes with rich pre-existing annotations from the Tree-of-Captions pipeline. Each video has temporal segments annotated at multiple levels of granularity:\n",
        "\n",
        "- **Level 0 (root/tier):** The full video — one annotation covering the entire 90-second clip\n",
        "- **Mid levels:** Sub-segments — multi-second chunks describing coherent activities\n",
        "- **Leaf level:** The finest grain — individual moments\n",
        "\n",
        "At each level, there are five annotation fields:\n",
        "- `gpt_summary_brief` — one-sentence clip caption\n",
        "- `gpt_summary_detailed` — full play-by-play description\n",
        "- `gpt_action_brief` — short verb phrase (\"spread almonds on tray\")\n",
        "- `gpt_action_detailed` — instruction-manual version of the action\n",
        "- `gpt_action_actor` — who's performing it\n",
        "\n",
        "All of these were generated by GPT-OSS-120B with three rounds of Self-Refine. We're going to take these at face value for now — and then build the tools to interrogate them.\n",
        "\n",
        "You can either download the dataset from the [Voxel51 Hugging Face org](https://huggingface.co/datasets/Voxel51/action100m_tiny_subset), or if you face rate limit issues, you can follow the instructions in the `download_scripts` directory of this repository to download the dataset and parse it into FiftyOne format.\n",
        "\n",
        "If you are downloading from the Hugging Face Hub, run:\n",
        "\n",
        "```python\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\n",
        "    \"Voxel51/action100m_tiny_subset\",\n",
        "    dataset_name=\"action100m\",\n",
        "    overwrite=True,\n",
        "    persistent=True,\n",
        ")\n",
        "```\n",
        "\n",
        "If you've downloaded the dataset and parsed manually, then run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/harpreet/miniconda3/envs/fo_video_workshop/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "dataset = fo.load_dataset(\"action100m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "471823d7",
      "metadata": {},
      "source": [
        "A **[FiftyOne dataset](https://docs.voxel51.com/user_guide/using_datasets.html)** is the core data structure you work with in [FiftyOne](https://docs.voxel51.com/user_guide/basics.html). It:\n",
        "\n",
        "- Logically represents your visual data (images, videos, point clouds, etc.) along with all associated information: labels, metadata, predictions, and other fields.\n",
        "\n",
        "- Is stored in a lightweight, non-relational database (MongoDB) so it can scale to large datasets without loading all media into RAM. \n",
        "\n",
        "- Can be created from many sources: directories of files, common dataset formats (like COCO), or the built‑in Dataset Zoo. \n",
        "\n",
        "Conceptually, you can [think of a dataset like a table in pandas](https://github.com/voxel51/fiftyone/blob/develop/docs/source/tutorials/pandas_comparison.ipynb): it has **rows and columns**, but specialized for computer vision:\n",
        "\n",
        "- In pandas: **row = record**, **column = feature**\n",
        "- In FiftyOne: **sample = record**, **field = feature** \n",
        "\n",
        "You can inspect the schema of a dataset by calling it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26754040",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7ab8fb",
      "metadata": {},
      "source": [
        "### What is a sample?\n",
        "\n",
        "A [**sample**](https://docs.voxel51.com/user_guide/basics.html#samples) is the atomic element of a FiftyOne dataset. It is:\n",
        "\n",
        "- One “item” of your data (for example, a single image or a single video) plus everything you know about it. \n",
        "- A flexible container of [**fields**](https://docs.voxel51.com/user_guide/basics.html#fields), which can include:\n",
        "  - `filepath` to the media\n",
        "  - `metadata` (dimensions, duration, etc.)\n",
        "  - Ground truth labels (detections, classifications, segmentations, etc.)\n",
        "  - Model predictions\n",
        "  - Tags, scalar values, strings, arrays, and more \n",
        "\n",
        "So, **a dataset is a collection of samples**, and **each sample is everything you care about for one media file**, stored in a structured way that’s easy to query, visualize, and modify.\n",
        "\n",
        "You can see what a sample looks like by inspecting the [first](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first) sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec0d626",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5436d44",
      "metadata": {},
      "source": [
        "Once you have your dataset, you can [launch the app](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app) and see what's in it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57635f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open the FiftyOne App and explore the dataset\n",
        "session = fo.launch_app(dataset, auto=False)\n",
        "session.url"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cad16bf7",
      "metadata": {},
      "source": [
        "**What to look at in the App:**\n",
        "\n",
        "1. Play a few videos and toggle on `gpt_action_brief` — watch how the temporal segments track the actions you see on screen\n",
        "2. Click on a sample with high `tree_depth` (try sorting by it) — notice how the annotations at level 0 are broad overviews, while leaf-level annotations are very fine-grained moments\n",
        "3. Look at the `transcript` field — this is the raw ASR text, much noisier than the GPT-refined annotations\n",
        "4. Check a few `gpt_summary_detailed` labels — these are the longest annotations (~540 words average)\n",
        "\n",
        "> **Key question to hold in mind:** Every one of these labels came from an automated pipeline. They look authoritative. But should we trust them?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0244b7",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 2: Three Lenses on the Same Data\n",
        "\n",
        "Before interrogating the existing annotations, let's build our own understanding of the dataset — using nothing but the raw videos and transcripts.\n",
        "\n",
        "We'll create three separate embedding spaces:\n",
        "\n",
        "1. **Visual ([Qwen3-VL-Embedding](https://docs.voxel51.com/plugins/plugins_ecosystem/qwen3vl_embeddings.html)):** What the videos look like — and crucially, this lives in a shared text-video space, so we can search with natural language\n",
        "\n",
        "2. **Visual-Grounding ([Molmo2](https://docs.voxel51.com/plugins/plugins_ecosystem/molmo2.html)):** A different visual understanding — video-to-video similarity only, but from a model trained on grounding and spatial reasoning\n",
        "\n",
        "3. **Language (Transcript):** What people are *saying* in these videos, embedded with a text model\n",
        "\n",
        "The insight from comparing these three spaces is the foundation of everything that follows: **what you embed determines what you find**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036f351b",
      "metadata": {},
      "source": [
        "### Lens 1: Visual Content (Qwen3-VL-Embedding)\n",
        "\n",
        "[Qwen3-VL-Embedding](https://huggingface.co/Qwen/Qwen3-VL-Embedding-2B) maps video and text into a shared vector space. This means we can embed a natural language query and find videos that match without touching any of the existing labels. It's semantic search, not keyword search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0af062de",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/harpreetsahota204/qwen3vl_embeddings...\n",
            "  121.6Mb [885.4ms elapsed, ? remaining, 137.3Mb/s] \n",
            "Overwriting existing model source '/home/harpreet/fiftyone/__models__/qwen3vl-embeddings'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 14 files: 100%|██████████| 14/14 [00:33<00:00,  2.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   0% ||--------------|    0/1144 [91.8ms elapsed, ? remaining, ? samples/s] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "qwen-vl-utils using decord to read video.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |███████████████| 1144/1144 [26.4m elapsed, 0s remaining, 0.8 samples/s]    \n",
            "Embeddings computed for 1144 samples\n"
          ]
        }
      ],
      "source": [
        "# Load the 2B embedding model — good balance of quality and speed\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/qwen3vl_embeddings\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "qwen_emb_model = foz.load_zoo_model(\n",
        "    \"Qwen/Qwen3-VL-Embedding-2B\",\n",
        ")\n",
        "\n",
        "qwen_emb_model.max_length=32768\n",
        "\n",
        "# Compute embeddings — stores a vector on each sample\n",
        "dataset.compute_embeddings(\n",
        "    qwen_emb_model,\n",
        "    embeddings_field=\"qwen_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=8,\n",
        "    skip_failures=False,\n",
        ")\n",
        "\n",
        "print(f\"Embeddings computed for {dataset.exists('qwen_embeddings').count()} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6907630f",
      "metadata": {},
      "source": [
        "### What you can do after computing embeddings in FiftyOne\n",
        "\n",
        "Once you've computed embeddings, you unlock powerful workflows:\n",
        "\n",
        "#### Visualize your embeddings in the App\n",
        "\n",
        "[Use `compute_visualization()`](https://docs.voxel51.com/brain.html#visualizing-embeddings) to reduce embeddings with UMAP, t‑SNE, or PCA and explore them in the Embeddings panel to:\n",
        "- See clusters and data structure  \n",
        "- Understand how classes group together  \n",
        "- Lasso regions to create views and filter subsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "33179678",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating visualization...\n",
            "UMAP( verbose=True)\n",
            "Mon Feb 23 13:16:41 2026 Construct fuzzy simplicial set\n",
            "Mon Feb 23 13:16:42 2026 Finding Nearest Neighbors\n",
            "Mon Feb 23 13:16:45 2026 Finished Nearest Neighbor Search\n",
            "Mon Feb 23 13:16:47 2026 Construct embedding\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed: 100%| ██████████ 500/500 [00:01]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  0  /  500 epochs\n",
            "\tcompleted  50  /  500 epochs\n",
            "\tcompleted  100  /  500 epochs\n",
            "\tcompleted  150  /  500 epochs\n",
            "\tcompleted  200  /  500 epochs\n",
            "\tcompleted  250  /  500 epochs\n",
            "\tcompleted  300  /  500 epochs\n",
            "\tcompleted  350  /  500 epochs\n",
            "\tcompleted  400  /  500 epochs\n",
            "\tcompleted  450  /  500 epochs\n",
            "Mon Feb 23 13:16:48 2026 Finished embedding\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<fiftyone.brain.visualization.VisualizationResults at 0x7a6ede3b8d50>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "# Project into 2D with UMAP for visualization\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"qwen_viz\",\n",
        "    embeddings=\"qwen_embeddings\",\n",
        "    num_dims=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db429e1d",
      "metadata": {},
      "source": [
        "\n",
        "####  Search by visual similarity\n",
        "\n",
        "[Build a similarity index](https://docs.voxel51.com/brain.html#similarity) with `compute_similarity()` to:\n",
        "- Find similar images (image‑to‑image search)  \n",
        "- Detect duplicates or unique samples  \n",
        "- Enable visual search in the App or via Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "93a14462",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<fiftyone.brain.internal.core.sklearn.SklearnSimilarityIndex at 0x7a6fa89a1190>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build a similarity index — this is what powers text-to-video search\n",
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    brain_key=\"qwen_sim\",\n",
        "    embeddings=\"qwen_embeddings\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "967bae59",
      "metadata": {},
      "source": [
        "You can also use this model for zero-shot classification. Let's add a Field to the Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "11ebd6a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |███████████████| 1144/1144 [26.3m elapsed, 0s remaining, 0.8 samples/s]    \n"
          ]
        }
      ],
      "source": [
        "classes = [\n",
        "    \"Cooking and Food\",\n",
        "    \"Home Improvement and DIY\",\n",
        "    \"Health and Beauty\",\n",
        "    \"Hobbies and Crafts\",\n",
        "    \"Sports and Fitness\",\n",
        "    \"Gardening\",\n",
        "    \"Technology and Electronics\",\n",
        "    \"Fashion and Style\",\n",
        "    \"Arts and Music\",\n",
        "    \"Automotive\",\n",
        "    \"Pets and Animals\",\n",
        "    \"Education and Learning\"\n",
        "]\n",
        "\n",
        "# Configure model for classification\n",
        "qwen_emb_model.classes = classes\n",
        "qwen_emb_model.text_prompt = \"A video about \"\n",
        "\n",
        "# Apply zero-shot classification\n",
        "dataset.apply_model(\n",
        "    qwen_emb_model, \n",
        "    label_field=\"predicted_class\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lens 2: Visual-Grounding (Molmo2)\n",
        "\n",
        "Molmo2 is a different kind of vision-language model — it's trained heavily on grounding tasks: pointing to objects, tracking them through time, and counting. Its internal representations will weight spatial and motion features differently than Qwen3-VL-Embedding.\n",
        "\n",
        "We can't do text-to-video search with Molmo2 embeddings, but we can still visualize the embedding space and do video-to-video similarity. \n",
        "\n",
        "The question is: **do two different visual models agree on which videos are similar?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz \n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/molmo2\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "molmo_model = foz.load_zoo_model(\"allenai/Molmo2-4B\")\n",
        "\n",
        "molmo_model.pooling_strategy = \"mean\"\n",
        "\n",
        "dataset.compute_embeddings(\n",
        "    molmo_model,\n",
        "    embeddings_field=\"molmo_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=4,\n",
        "    skip_failures=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"molmo_viz\",\n",
        "    embeddings=\"molmo_embeddings\",\n",
        "    num_dims=2,\n",
        ")\n",
        "\n",
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    brain_key=\"molmo_sim\",\n",
        "    embeddings=\"molmo_embeddings\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lens 3: Language (Transcript Embeddings)\n",
        "\n",
        "Both visual embeddings above represent what you *see* in the videos. Now let's embed what you *hear* — the ASR transcript text.\n",
        "\n",
        "Instructional videos vary enormously in how much they narrate. Some presenters talk through every step; others work in silence. The transcript embedding space captures this variation in a way neither visual model can.\n",
        "\n",
        "Fo this we can make use of [`jina-embeddings-v5-text-small-clustering`](https://huggingface.co/jinaai/jina-embeddings-v5-text-small-clustering). \n",
        "\n",
        "This model is not integrated as a remote source zoo model, but we can make use of it by extracting the transcripts from the videos using the `values` method of the [Dataset](https://docs.voxel51.com/user_guide/basics.html#datasets) to get all the values of the [Field](https://docs.voxel51.com/user_guide/basics.html#fields) into a Python list.\n",
        "\n",
        "With the values in a list we can use the model natively and then add the results back as a Field to the Dataset using [the `set_values`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.set_values) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Default prompt name is set to 'document'. This prompt will be applied to all `encode()` calls, except if `encode()` is called with `prompt` or `prompt_name` parameters.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#set an environment variable so tokenizers doesn't yell at us,\n",
        "# note this related to the `transformers` and `tokenizers` libraries and not a FiftyOne specific environment variable\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "text_emb_model = SentenceTransformer(\n",
        "    \"jinaai/jina-embeddings-v5-text-small-clustering\",\n",
        "    model_kwargs={\"dtype\": torch.bfloat16}, #recommended for GPUs\n",
        "    config_kwargs={\"attn_implementation\": \"flash_attention_2\"}, #optional, but recommended\n",
        "    tokenizer_kwargs={\"extra_special_tokens\": {}}, # This line fixes the AttributeError: 'list' object has no attribute 'keys'\n",
        ")\n",
        "\n",
        "\n",
        "transcripts = dataset.values(\"transcript\")\n",
        "\n",
        "# Encode texts\n",
        "text_embeddings = text_emb_model.encode(transcripts)\n",
        "\n",
        "dataset.set_values(\"text_embeddings\", text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9f245ca5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating visualization...\n",
            "UMAP( verbose=True)\n",
            "Mon Feb 23 14:26:10 2026 Construct fuzzy simplicial set\n",
            "Mon Feb 23 14:26:10 2026 Finding Nearest Neighbors\n",
            "Mon Feb 23 14:26:10 2026 Finished Nearest Neighbor Search\n",
            "Mon Feb 23 14:26:10 2026 Construct embedding\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  49%| ████▉      246/500 [00:00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  0  /  500 epochs\n",
            "\tcompleted  50  /  500 epochs\n",
            "\tcompleted  100  /  500 epochs\n",
            "\tcompleted  150  /  500 epochs\n",
            "\tcompleted  200  /  500 epochs\n",
            "\tcompleted  250  /  500 epochs\n",
            "\tcompleted  300  /  500 epochs\n",
            "\tcompleted  350  /  500 epochs\n",
            "\tcompleted  400  /  500 epochs\n",
            "\tcompleted  450  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed: 100%| ██████████ 500/500 [00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Feb 23 14:26:10 2026 Finished embedding\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<fiftyone.brain.visualization.VisualizationResults at 0x7a70f44f6750>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"transcript_viz\",\n",
        "    embeddings=\"text_embeddings\",\n",
        "    num_dims=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e64caaf",
      "metadata": {},
      "source": [
        "We can compute visualizations of the embeddings just like before:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6cb543b",
      "metadata": {},
      "source": [
        "### Using Embeddings to Compute Uniqueness and Representativeness Values\n",
        "\n",
        "In FiftyOne, both **uniqueness** and **representativeness** are scalar scores computed from embeddings that describe how a sample relates to the rest of the dataset.\n",
        "\n",
        "#### Uniqueness\n",
        "\n",
        "- [Measures **how different (dissimilar)**](https://docs.voxel51.com/brain.html#image-uniqueness) a sample is from its neighbors in embedding space.  \n",
        "\n",
        "- Implemented by looking at each sample’s nearest neighbors, weighting their distances (e.g. 60%-30%-10%), and normalizing to a score in **\\[0, 1\\]**. Higher scores mean the sample is more “isolated” or distinct; lower scores mean it has many close neighbors.\n",
        " \n",
        "- The most unique sample in a collection has a uniqueness value of **1**.\n",
        "\n",
        "- Useful for:\n",
        "  - Finding **outliers / edge cases / bad actors**  \n",
        "  - Detecting **near-duplicates** (by looking at *low* uniqueness)  \n",
        "  - Selecting **diverse samples** for annotation when budget is limited \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "29ae91c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing uniqueness...\n",
            "Uniqueness computation complete\n"
          ]
        }
      ],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_uniqueness(\n",
        "    dataset,\n",
        "    uniqueness_field = \"qwen_uniqueness\",\n",
        "    embeddings = \"qwen_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fcd344b",
      "metadata": {},
      "source": [
        "## Representativeness\n",
        "\n",
        "- [Measures **how typical** or **central** ](https://docs.voxel51.com/brain.html#image-representativeness) a sample is relative to the rest of the dataset in embedding space.  \n",
        "\n",
        "- Also normalized to **\\[0, 1\\]**, with the most representative samples having value **1**.  \n",
        "\n",
        "- High representativeness ≈ sample lies in a dense, central region of the data; it’s similar to many other samples.  \n",
        "\n",
        "- Useful for:\n",
        "  - **Active learning**: picking representative samples to cover common patterns  \n",
        "  - **Dataset balancing**: finding under/overrepresented regions  \n",
        "  - **Efficient annotation**: prioritizing samples that “stand in” for many others \n",
        "\n",
        "In short:\n",
        "\n",
        "- **Uniqueness** → “How unusual is this sample?”  \n",
        "- **Representativeness** → “How well does this sample represent many others?”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "1ad16d93",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing representativeness...\n",
            "Computing clusters for 1144 embeddings; this may take awhile...\n",
            "Representativeness computation complete\n"
          ]
        }
      ],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_representativeness(\n",
        "    dataset,\n",
        "    representativeness_field = \"qwen_rep\",\n",
        "    embeddings = \"qwen_embeddings\",\n",
        "    batch_size=64,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Cross-Lens Moment\n",
        "\n",
        "Try to **find a tight visual cluster in the Qwen UMAP, then look at where those same videos land in the transcript UMAP.**\n",
        "\n",
        "You may find that videos which cluster tightly by visual content are often spread out in transcript space. Cooking videos that all look similar — same kitchen setup, same hands-on-food visual — may use completely different spoken language: detailed step-by-step narration, casual conversational chat, or total silence.\n",
        "\n",
        "This is not a failure of any embedding model, but a property of the data: **visual similarity and linguistic similarity are measuring different things.** Your choice of embedding determines what questions you can ask.\n",
        "\n",
        "- Qwen visual embeddings to find *visually similar content* or do text-to-video semantic search\n",
        "\n",
        "- Molmo2 embeddings to find videos with similar *physical actions and spatial structure*\n",
        "\n",
        "- Transcript embeddings to find videos that *talk about similar topics*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick a video and find its nearest neighbors in each embedding space — \n",
        "# a concrete way to see how the three spaces differ\n",
        "sample = dataset.first()\n",
        "print(f\"Reference video: {sample.title}\")\n",
        "print(f\"Root annotation: {sample.gpt_root_summary}\\n\")\n",
        "\n",
        "for brain_key, label in [(\"qwen_sim\", \"Qwen visual\"), (\"molmo_sim\", \"Molmo2 visual\")]:\n",
        "    neighbors = dataset.sort_by_similarity(sample.id, brain_key=brain_key, k=4)\n",
        "    print(f\"Nearest neighbors ({label}):\")\n",
        "    for n in neighbors.skip(1).take(3):\n",
        "        print(f\"  → {n.gpt_root_summary}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e305612",
      "metadata": {},
      "source": [
        "**What to look at:** \n",
        "\n",
        "- In the Embeddings panel, hover over clusters. Click points to see which videos land near each other. Try coloring by `tree_depth` or `title` to see if the visual clusters map onto any metadata patterns.\n",
        "\n",
        "\n",
        "- Compare the Molmo2 UMAP with the Qwen UMAP. Are the clusters in the same positions? Similar shape, different boundaries — this is two models with different visual priors producing different embedding spaces. Neither is \"correct\" — they're measuring different aspects of the same content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 3: The Second Opinion\n",
        "\n",
        "We've built three independent maps of the dataset. Now let's do what the original pipeline did — but with a completely different model.\n",
        "\n",
        "The Action100M annotations were produced by GPT-OSS-120B processing outputs from PerceptionLM-3B and Llama-3.2-Vision-11B. \n",
        "\n",
        "We're going to run **Qwen3-VL-8B** on the same videos, completely independently, and ask:\n",
        "\n",
        "- How would *you* describe this video?\n",
        "\n",
        "- What events do *you* see, and when?\n",
        "\n",
        "We're not trying to beat the pipeline. We're trying to get a second opinion. Where the two agree, we gain confidence in the original annotation. Where they diverge, we've found something worth looking at."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz \n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/qwen3vl_video\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "qwen_video_model = foz.load_zoo_model(\n",
        "    \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a full-video description for every sample.\n",
        "# This produces a string field: qwen_desc_summary\n",
        "qwen_video_model.operation = \"description\"\n",
        "\n",
        "dataset.apply_model(\n",
        "    qwen_video_model,\n",
        "    label_field=\"qwen_desc\",\n",
        "    skip_failures=True,\n",
        ")\n",
        "\n",
        "print(f\"Descriptions generated: {dataset.exists('qwen_desc_summary').count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporally localize events — produces TemporalDetections with start/end frames.\n",
        "# Compare these against gpt_action_brief in the App.\n",
        "qwen_video_model.operation = \"temporal_localization\"\n",
        "\n",
        "dataset.apply_model(\n",
        "    qwen_video_model,\n",
        "    label_field=\"qwen_events\",\n",
        "    skip_failures=True,\n",
        ")\n",
        "\n",
        "print(f\"Temporal events generated: {dataset.exists('qwen_events').count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice the descriptions often describe the same video in quite different language — different vocabulary, different sentence structure, different level of detail. Sometimes one catches a detail the other misses. \n",
        "\n",
        "The question is: **how do we move from \"these look different\" to a number we can sort and filter by?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 4: Measuring Agreement\n",
        "\n",
        "Eyeballing agreement doesn't scale. We need a metric.\n",
        "\n",
        "The obvious choice is a lexical similarity metric: edit distance, word overlap, something that compares the actual characters and words in the two strings. \n",
        "\n",
        "For example:\n",
        "- GPT might say: *\"spread almonds on tray\"*  \n",
        "- Qwen might say: *\"person distributes nuts onto a baking sheet\"\n",
        "\n",
        "These are describing exactly the same action. But their character-level edit distance is enormous — they share almost no words. Any lexical metric will call this a bad match.\n",
        "\n",
        "This is why the right metric for comparing paraphrased descriptions is **not** edit distance. But edit distance is still *useful* — it gives us a lower bound, and the gap between lexical and semantic similarity is itself informative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.operators as foo \n",
        "\n",
        "# ANLS: Average Normalized Levenshtein Similarity\n",
        "# Standard metric for OCR/VLM evaluation — robust to minor errors,\n",
        "# but still character-level. Watch how it penalizes semantic equivalents.\n",
        "anls_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_anls\")\n",
        "\n",
        "anls_result = anls_op(\n",
        "    dataset,\n",
        "    pred_field=\"qwen_desc_summary\",\n",
        "    gt_field=\"gpt_root_summary\",\n",
        "    output_field=\"anls_score\",\n",
        "    threshold=0.5,\n",
        "    case_sensitive=False,\n",
        "    delegate=False,\n",
        ")\n",
        "\n",
        "print(f\"Mean ANLS: {anls_result.get('mean_anls', 'N/A'):.3f}\")\n",
        "print(\"(Low score expected — lexical metrics penalize paraphrase)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalized Levenshtein Similarity — continuous 0–1 score without threshold\n",
        "# More granular than ANLS, still lexical.\n",
        "sim_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_normalized_similarity\")\n",
        "\n",
        "sim_result = sim_op(\n",
        "    dataset,\n",
        "    pred_field=\"qwen_desc_summary\",\n",
        "    gt_field=\"gpt_root_summary\",\n",
        "    output_field=\"annotation_confidence\",\n",
        "    case_sensitive=False,\n",
        "    delegate=False,\n",
        ")\n",
        "\n",
        "print(f\"Mean normalized similarity: {sim_result.get('mean_similarity', 'N/A'):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What to look at:** Browse the bottom of the sorted view (lowest confidence). Are these actually bad annotations, or just paraphrases that look different? This is the limitation of lexical metrics — they can't tell the difference. A future extension here is a semantic similarity operator using sentence embeddings, which would score paraphrases much more accurately.\n",
        "\n",
        "For now, the low-confidence samples are exactly what we want for the next section: the cases where the two systems most clearly diverge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 5: Grounding the Hard Cases\n",
        "\n",
        "We now have a ranked list of the samples where GPT-OSS-120B and Qwen3-VL disagree most strongly. Instead of reading more text descriptions, let's ask a different question: **can a third model show us what's actually happening?**\n",
        "\n",
        "Molmo2's core strength is grounding — it can point to specific objects and actors in frames, and it can localize when events happen over time. When two models disagree in words, Molmo2 provides a qualitatively different kind of evidence: spatial coordinates in the actual pixels.\n",
        "\n",
        "This isn't just a tiebreaker. It demonstrates a principle: **text descriptions are abstractions. When abstractions conflict, go back to the source.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Work on the bottom quartile — the most uncertain samples\n",
        "# threshold = sorted(scores)[len(scores) // 4]  # 25th percentile\n",
        "# uncertain_view = dataset.match(F(\"annotation_confidence\") < threshold)\n",
        "# print(f\"Working with {len(uncertain_view)} low-confidence samples (score < {threshold:.2f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Temporal localization with Molmo2 — independent from both previous models\n",
        "# molmo_model.operation = \"temporal_localization\"\n",
        "\n",
        "# uncertain_view.apply_model(\n",
        "#     molmo_model,\n",
        "#     label_field=\"molmo_events\",\n",
        "#     skip_failures=True,\n",
        "# )\n",
        "\n",
        "# print(f\"Molmo2 temporal localization done for {uncertain_view.exists('molmo_events').count()} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pointing — ask Molmo2 to locate the main actor on each frame\n",
        "# This produces frame-level keypoints in the App\n",
        "molmo_model.operation = \"pointing\"\n",
        "molmo_model.prompt = \"the person performing the main action\"\n",
        "\n",
        "uncertain_view.apply_model(\n",
        "    molmo_model,\n",
        "    label_field=\"molmo_actor\",\n",
        "    skip_failures=True,\n",
        ")\n",
        "\n",
        "print(f\"Molmo2 pointing done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the uncertain samples with all three annotation sources visible:\n",
        "# gpt_action_brief (original), qwen_events (second opinion), molmo_events (third opinion)\n",
        "# molmo_actor keypoints are frame-level — visible in the video frame panel\n",
        "session.view = uncertain_view.exists(\"molmo_events\")\n",
        "\n",
        "print(\"In the App, for each uncertain sample, look at:\")\n",
        "print(\"  Timeline: gpt_action_brief | qwen_events | molmo_events\")\n",
        "print(\"  Frames:   molmo_actor (keypoints showing where the actor is)\")\n",
        "print(\"  Text:     gpt_root_summary | qwen_desc_summary\")\n",
        "print()\n",
        "print(\"Cases to find:\")\n",
        "print(\"  ✓ Molmo2 agrees with one model — tiebreaker worked\")\n",
        "print(\"  ? All three diverge — the video is genuinely ambiguous\")\n",
        "print(\"  ✗ Low score was just a paraphrase — lexical metric was wrong\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 6: The Payoff\n",
        "\n",
        "Let's look at what we built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fiftyone import ViewField as F \n",
        "\n",
        "# High-confidence subset: both models agree — ready to use\n",
        "high_confidence = dataset.match(F(\"annotation_confidence\") >= 0.6)\n",
        "print(f\"High-confidence subset (score ≥ 0.6): {len(high_confidence)} samples\")\n",
        "\n",
        "# Review queue: models disagree — worth a closer look\n",
        "review_queue = dataset.match(F(\"annotation_confidence\") < 0.3)\n",
        "print(f\"Review queue (score < 0.3):           {len(review_queue)} samples\")\n",
        "\n",
        "# Open the full dataset with everything visible\n",
        "session.view = dataset\n",
        "print(\"\\nFull dataset loaded in App. Use Filter to slice by annotation_confidence.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What We Built\n",
        "\n",
        "We started with 1,144 videos and a set of annotations we had to take on faith.\n",
        "\n",
        "We ended with a **confidence map**:\n",
        "\n",
        "- **High confidence (`annotation_confidence ≥ 0.6`)** — both GPT-OSS-120B and Qwen3-VL describe these clips similarly. The annotation is likely reliable. Use these for training or evaluation.\n",
        "\n",
        "- **Low confidence (`annotation_confidence < 0.3`)** — the models diverge. Some of these are just paraphrase failures (a limitation of lexical metrics). Others are genuinely ambiguous clips where even multiple frontier models don't agree. These are your review queue — and Molmo2's spatial grounding is your best tool for investigating them.\n",
        "\n",
        "---\n",
        "\n",
        "### The Workflow, Generalized\n",
        "\n",
        "Everything we did here transfers directly to any video dataset with AI-generated labels:\n",
        "\n",
        "1. **Load and explore** — understand the annotation structure before trusting it\n",
        "2. **Embed from multiple angles** — visual content, visual grounding, language; each lens reveals something different\n",
        "3. **Generate an independent second opinion** — run a different model, compare outputs\n",
        "4. **Quantify agreement** — a score per sample turns qualitative inspection into a sortable, filterable signal\n",
        "5. **Ground the disagreements** — when text descriptions conflict, spatial evidence is the tiebreaker\n",
        "\n",
        "The tools: **FiftyOne** for orchestration and visualization, **Qwen3-VL-Embedding** for the searchable visual index, **Qwen3-VL** for independent annotation, **Molmo2** for grounding, and a **text evaluation plugin** to quantify it all.\n",
        "\n",
        "The dataset doesn't matter. The workflow does."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fo_video_workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
